{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestApp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0X7EB7Gd4qpCJ9mV/7N6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samoed/CourseWork2/blob/main/TestApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlBuf27tCgos",
        "outputId": "ce51a7fb-8039-4d09-ded4-7e79b79c285c"
      },
      "source": [
        "!pip install -U bert-extractive-summarizer\n",
        "!pip install -U transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/07/fdb05f9e18b6f641499ef56737126fbd2fafe1cdc1a04ba069d5aa205901/bert_extractive_summarizer-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.7.2)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.9MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=280162c33712d2c5ca87d16c2d9777aacc6aa351387b16474f12930b9844bb44\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.7.1 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n",
            "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk0khuLmERs1"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from tqdm import tqdm\n",
        "from summarizer import Summarizer\n",
        "\n",
        "import pickle\n",
        "import re\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTdynMc6EWeD"
      },
      "source": [
        "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "custom_config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "custom_config.output_hidden_states=True\n",
        "custom_tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "custom_model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased', config=custom_config)\n",
        "#custom_model.to('cuda')\n",
        "custom_model.eval()\n",
        "#torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AenS0TmXEgqD"
      },
      "source": [
        "modelSum = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "j3qbJ1LtFZZf",
        "outputId": "c5c9c140-6405-4c28-c633-60ab1a4c47d5"
      },
      "source": [
        "with open(\"/content/TestDoc\", 'r') as f:\n",
        "    text = f.read()\n",
        "    text = \"\".join([i.replace('\\n',' ').replace('\\t',' ') for i in text])\n",
        "text\n",
        "embeddings = modelSum.run_embeddings(text, num_sentences=10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Пермский филиал федерального государственного автономного образовательного учреждения высшего образования  «Национальный исследовательский университет  «Высшая школа экономики»   Факультет экономики, менеджмента и бизнес-информатики   Абросимова Полина Сергеевна   РАЗРАБОТКА СРЕДСТВ АВТОМАТИЗАЦИИ РАСШИРЕНИЯ ОНТОЛОГИИ НА ОСНОВЕ ДАННЫХ ИНТЕРНЕТ-ИСТОЧНИКОВ  Выпускная квалификационная работа  по направлению подготовки 38.03.05\\xa0Бизнес-информатика образовательная программа «Бизнес-информатика»      К.ф.-м.н., доцент, ПГНИУ,    Руководитель доцент кафедры       к.ф.-м.н, доцент, доцент информационных технологий    кафедры Л.А. Залогова      информационных         технологий в бизнесе         __________________________         Л.Н. Лядова             Пермь, 2019 год Аннотация В данной работе совершенствуется система мониторинга глобальных процессов сети Интернет. Приводится обоснование необходимости пополнения существующей онтологии, а также ведётся анализ трёх основных методов пополнения онтологии: пополнение онтологии вручную, интеграция двух существующих онтологий и автоматическое пополнение онтологии. В ходе анализа был выбран метод автоматического пополнения онтологии, так как в данном случае отсутствует необходимость в эксперте предметной области и инженере онтологии, а также данный метод позволит своевременно обновлять онтологию на основе найденных данных в сети Интернет по запросу пользователя. Также был выбран формат представления онтологии turtle. Был спроектирован алгоритм автоматического пополнения онтологии с помощью диаграммы нотации «Процесс» и метаязыка, описывающего структуру для реализации алгоритма. Для разработки были использованы инструменты JavaScript, WebStorm IDE, библиотеки NightMare, SPARQL.js, retext-keywords. Был разработан алгоритм поиска часто встречаемых ключевых слов, а также проверка наличия этих слов в онтологии. Работа изложена в трёх главах на 36 листах, включает в себя 8 рисунков, две таблицы, три приложения.  Оглавление Введение 4 Глава 1. Анализ существующих алгоритмов расширения онтологии 8 1.1. Результаты работы за предыдущий год 8 1.2. Постановка задачи 9 1.3. Методы расширения онтологии 11 1.3.1. Пополнение онтологии вручную 11 1.3.2. Интеграция двух онтологий 13 1.3.3. Автоматическое пополнение онтологии 16 1.3.4. Сравнение методов расширения онтологии 17 1.4. Форматы представления онтологии 18 1.5. Выводы 21 Глава 2. Проектирование сервиса 23 2.1. Проектирование алгоритма 23 2.2. Источники данных 25 2.3. Выводы 28 Глава 3. Разработка автоматического пополнения онтологии 29 3.1. Обоснование выбора инструментов разработки 29 3.2. Изменение источника данных 30 3.3. Реализация алгоритма автоматического                  пополнения онтологии 32 Заключение 35 Библиографический список 37 Приложение А. Техническое задание 40 Приложение Б. Исходный код алгоритма поиска                            ключевых слов в тексте 46 Приложение В. Исходный код поиска                             отсутствующих в онтологии ключевых слов 47  Введение С появлением сети Интернет количество доступной людям информации увеличивается стремительными темпами. Поисковые системы выдают тысячи результатов на запрос пользователя, однако информация предоставляется пользователю в необработанном виде. В связи с этим возникает ряд проблем. Для поиска необходимой информации пользователи тратят время, которое зачастую может быть ограничено различными факторами. Ещё больше времени может уйти на структурирование информации, в связи с чем какие-то данные могут быть искажены или потеряны.  Отдельные данные пользователь может пропустить по ряду причин, например, рассеянности внимания или большого количества информации. Также большое влияние на качество информации оказывает источник данных. Важна корректная формулировка запроса: по различным запросам можно получить разные результаты поиска. Пользователь может не принимает этот факт во внимание или даже не знать о подобных возможностях поиска. Пользователь может некорректно сформировать запрос, из-за чего часть необходимых данных может быть потеряна.  Отдельная проблема заключается в том, что в связи с субъективным восприятием пользователя информация может быть искажена. С целью решения указанных выше проблем возникла необходимость разработать систему, структурирующую информацию и предоставляющую результаты в виде, удобном для пользователя [2, 4, 5, 7]. На данный момент уже существуют системы, позволяющие решить поставленные проблемы. Однако, в подобных системах существуют свои недостатки [3, 6].  Существуют средства расширенного поиска, позволяющие задать синонимы, допустимые слова или фразы, которые не стоит включать в поиск и так далее, однако связи между словами в подобном случае не учитываются [1, 7, 10]. Данные средства могут расширить результаты поиска, предоставив больше необходимой информации, однако остаётся проблема неструктурированных данных. Существуют отдельные решения для анализа данных, например, RapidMiner\\xa0[24], однако программы перегружены функциональными возможностями и достаточно сложны для понимания непрофессионального пользователя. Более того, подобные решения платные, что затрудняет их использование. Таким образом, создание системы автоматизированного поиска и структурирования данных весьма актуально, поскольку не существует прямых бесплатных аналогов разрабатываемой системы, которая могла бы решать все описанные выше проблемы. Разработке подобной системы была посвящена курсовая работа предыдущего года. В работе описаны задачи, выполнение которых необходимо для решения поставленных проблем, а именно:     1. Расширение пользовательского запроса. На основе введённого пользователем запроса необходимо формулировать различные запросы, позволяющие охватить больше информации по заданной теме.     2. Получение результатов из различных поисковых источников по запросам, поскольку поисковые роботы разных поисковых систем работают по разным алгоритмам, в связи с чем результаты могут сильно отличаться.     3. Извлечение результатов поиска для работы с данными.     4. Структурирование данных.     5. Представление данных в виде модели глобального процесса для пользователя. В разработанной системе были выполнены пункты 3 и 4. Извлечение данных ведётся при помощи парсинга по HTML-тегам страницы; сохраняются и отображаются только отобранные данные. Для структурирования данных было решено использовать онтологию предметной области. Структурирование данных представляет из себя создание элементарных запросов к онтологии по результатам поиска. Ввиду коротких сроков работы над проектом созданная система имеет весьма ограниченные функциональные возможности, которые необходимо дорабатывать для достижения лучших результатов. Данная работа является продолжением упомянутой выше. Проблема, которая решается в данной работе, связана с расширением онтологии предметной области. Используемая онтология может оказаться неполной, в связи с чем данные, необходимые пользователю, могут быть утеряны на этапе структуризации данных. При анализе результатов, полученных по запросу, может появиться необходимость добавить какие-либо слова или фразы в онтологию. Расширение онтологии необходимо автоматизировать. На данный момент уже существуют различные методы и средства расширения онтологии. Объектом исследования являются методы проектирования средств автоматизации онтологии, внедрённой в систему мониторинга глобальных бизнес-процессов. Предметом исследования – средства автоматизации расширения онтологии. Целью работы является разработка и внедрение в разрабатываемую систему оптимального алгоритма расширения онтологии выбранной предметной области. Для достижения поставленной цели был выдвинут ряд задач:     1. Анализ существующих методов расширения онтологии.     2. Анализ форматов представления онтологии.     3. Разработка алгоритма расширения онтологии. В предыдущей работе были использованы методы web-mining и text-mining для извлечения данных из сети Интернет. В данной работе для индексации текста изучаются дополнительные возможности данных методов. Для разработки алгоритма автоматического расширения онтологии предлагается использование диаграмм нотации «Процесс» и метаязыка. С помощью функционального программирования алгоритм был внедрён в систему. Тестирование корректной работы системы производилось с использованием тестирования чёрного ящика. В первой главе рассматриваются существующие методы расширения онтологии. На основе проанализированных решений делается вывод о наиболее подходящем методе расширения онтологии выбранной предметной области.  Также в первой главе делается выбор о формате, в котором будет храниться онтология. Для этого ведётся подробный разбор трёх форматов представления онтологии. Во второй главе представлены результаты разработки выбранного алгоритма расширения онтологии. Описаны результаты проектирования алгоритма для расширения онтологии, а также разобраны источники данных для анализа текстов. В третьей главе приводится обоснование выбора инструментов разработки. Описаны этапы реализации алгоритма автоматизации в рамках разрабатываемой системы, а также приведены результаты тестирования алгоритма. Глава 1. Анализ существующих алгоритмов расширения онтологии Для проектирования алгоритма расширения онтологии необходимо выполнить исследование по следующим темам:     1. Рассмотрение методов расширения онтологии.     2. Рассмотрение форматов представления онтологии. На основе анализа изученных методов формируется общее представление разрабатываемого алгоритма для расширения онтологии.  В данной главе описывается поставка задачи, а также рассматриваются методы расширения и представления онтологий. В ходе исследования были изучены научные статьи на тему расширения онтологии. 1.1. Результаты работы за предыдущий год В результате курсовой работы прошлого года была разработана система мониторинга глобальных процессов, целью которой было упрощение поиска важной информации и структурирование её для непрофессионального пользователя. Поиск проводился в поисковой системе Google. На систему было наложено ограничение по использованию онтологии для хранения данных о предметной области. В связи с этим был проведён анализ возможных методов и средств для реализации требуемых функциональных возможностей на основе онтологии. В результате анализа методов и средств извлечения данных был реализован парсер для извлечения данных по HTML-разметке web-страницы. Структуризация данных заключается в создании SPARQL-запросов к онтологии предметной области. Для реализации системы на основании выбранных методов и средств извлечения и структуризации данных был выбран язык программирования JavaScript, а сама разработка велась в среде программирования WebStorm. Был реализован web-клиент, позволяющий пользователю вводить запрос, с помощью которого парсер обращался к поисковой системе Google, имитировал нажатие клавиш для получения результатов, а после переходил в новостной раздел и извлекал по тегам необходимые данные. Далее заголовки и тексты новостей были разбиты по словам и проанализированы с помощью SPARQL-запросов к онтологии. На данный момент результаты выводятся только при наличии слов в форме, в которой они находятся в онтологии. Таким образом, для новости (см. рис. 1.1) со словом «Разлив», которое встречается в онтологии и имеет экземпляры «Разлив» и «произошла_утечка», были выведены результаты: {\"date\":\"27 февр. 2018 г.\", \"source\":\"Ridus.ru\",\"title\":[],\"text\":[[{\"entity\":{\"token\":\"uri\",\"value\":\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Разлив\"}},{\"entity\":{\"token\":\"uri\",\"value\":\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#произошла_утечка\"}}]]}.  Рисунок 1.1. Новость по запросу «Разлив нефти в США» На данный момент разработанная система мониторинга глобальных процессов работает следующим образом:     1. Пользователь вводит запрос и нажимает клавишу «Поиск».     2. Программа передаёт запрос поисковой системе и запускает поиск.     3. Извлекаются данные результатов поиска на первой странице.     4. Данные структурируются путём создания простых запросов к онтологии выбранной предметной области.     5. Отсортированные результаты выводятся пользователю. 1.2. Постановка задачи В данной работе ведётся совершенствование разрабатываемой системы. Как уже было описано выше, созданная система структурирует данные путём создания SPARQL-запросов к онтологии. Недостаток такого подхода заключается в том, что подходящее слово может не оказаться в онтологии, в результате чего отдельные данные по предметной области могут быть упущены программой и не выведены пользователю. Подобное может произойти по причине того, что при построении онтологии не была учтена возможность использования данного слова в контексте выбранной предметной области или же это новое слово или фраза, которые появились относительно недавно и не были занесены в онтологию. Одним из решений данной проблемы является оценка результатов поиска и расширение онтологии новыми понятиями. Под расширением онтологии подразумевается пополнение онтологии новыми понятиями, полученными на основе результатов поиска. Целью работы является разработка и внедрение в существующую систему оптимального алгоритма расширения онтологии выбранной предметной области на основе результатов поиска в Интернет-источниках. К алгоритму предъявляются следующие требования:     1. Необходимо проанализировать результаты поиска. В случае, если какое-либо слово или фраза встречается с определённой периодичностью и отсутствует в онтологии, необходимо предусмотреть возможность добавления этого слова или фразы в онтологию.     2. Если найдено слово или фраза, которые было решено добавить в онтологию, необходимо оценить существующие связи в онтологии, чтобы определить возможную взаимосвязь найденного слова или фразы с остальными понятиями в онтологии.     3. Предложить пользователю возможность добавить определённое слово в онтологию и в случае положительного ответа пополнить онтологию. Ограничений у данной работы несколько. Во-первых, важно учитывать, что алгоритм необходимо будет интегрировать в уже существующую систему. Реализация должна быть выполнена с учётом выбранных для разработки системы технологий. Во-вторых, выбранная библиотека SPARQL.js допускает работу с онтологией в формате .ttl. Необходимо учесть формат онтологии при проектировании алгоритма. 1.3. Методы расширения онтологии Расширение онтологии подразумевает дополнение онтологии сущностями и связями. Различные методы расширения онтологий изучались и совершенствовались со временем. В данном разделе будут рассмотрены некоторые из них. По теме расширения онтологии был проведён обзор существующих статей. 1.3.1. Пополнение онтологии вручную Один из самых технически простых способов расширения онтологии – пополнить её вручную. Этот метод подходит для случаев, когда инженер предметной области заинтересован в поддержании актуальности онтологии и имеет прямой доступ к ней. Обычно этот метод используется, когда не нужно вносить большие изменения или при анализе других методов было решено, что эффективнее и выгоднее пополнять онтологию вручную. На тему ручного расширения онтологии была изучена статья Габриэллы Симон-Надь и Риты Флейнер [8]. Авторы статьи использовали онтологию для хранения плана зданий, навигации между ними и хранение атрибутов специальной техники людей с ограниченными возможностями. Разработанная онтология под названием iLOC должна послужить для составления возможных оптимальных путей перемещения людей с ограниченными возможностями. Подобные онтологии уже были созданы, однако перед авторами данной статьи стояла цель как можно более точно определить маршрут для людей с конкретными возможностями. Авторы статьи предлагают использовать более детальные характеристики оборудования для людей с ограниченными возможностями для поиска более точных и безопасных маршрутов. В пример авторы статьи приводят инвалидные коляски, которые могут иметь разную структуру, разный диаметр колеса, разные механизмы, используемые для движения. Авторы сосредоточились на том, чтобы в поиске наиболее подходящего маршрута учитывать всевозможные параметры, которые могут либо расширить результаты поиска, либо сократить, но указать наиболее точные возможные маршруты для конкретных параметров. Авторы статьи выбирают ручное пополнение онтологии по нескольким причинам. Во-первых, указание корректных атрибутов напрямую связано с безопасностью человека, допущение ошибок в данном случае недопустимо. Чтобы снизить вероятность указания не тех параметров, авторы выбирают ручное пополнение онтологии. Во-вторых, пополнение онтологии в конкретном случае необходимо совершить единожды для демонстрации того, как может улучшиться поиск возможного пути, если учитывать критерии оборудования для людей с ограниченными возможностями. Таким образом, авторы статьи создали расширение под названием iACC для выбранной онтологии. Добавление сущностей вручную позволило авторам расширить онтологию в короткие сроки. Ручное пополнение онтологии не являлось проблемой, поскольку сделать это нужно было единожды. Недостатком такого метода является необходимость привлечения эксперта предметной области, который сформирует необходимые сущности и связи, исходя из существующей онтологии. Также важно привлечение инженера онтологии, который вносит информацию в онтологию. Однократное пополнение онтологии может быть выгодно совершить вручную, однако использование этого метода периодически может привести к высоким трудозатратам. В отдельных случаях способ ручного пополнения онтологии весьма эффективен. Учитывая поставленную в данной работе задачу, пополнение онтологии вручную может привести к ряду проблем. Во-первых, в данной работе одна онтология предметной области приведена для демонстрации возможностей системы. В перспективе пользователь сам выбирает онтологию необходимой ему предметной области или же система сама учитывает, какую онтологию нужно выбрать, исходя из формулировки запроса к поисковой системе. Следить за каждой онтологией будет весьма тяжело, если их будет несколько десятков, а то и больше. Во-вторых, в случае существования нескольких онтологий, сложно поддерживать актуальное состояние каждой онтологии. Возможно периодическое привлечение экспертов предметной области для оценки актуальности онтологии за отдельную плату. Однако в таком случае появятся расходы, которые не предусмотрены в разработке данной системы. В-третьих, если учитывать привлечение экспертов предметной области, между оценками актуальности онтологии будет проходить какое-то время, например, месяц. При появлении какого-то важного понятия в этот промежуток существует вероятность, что пользователь не получит важной информации, поскольку онтология не учтёт указанное слово и не выдаст необходимый результат. Таким образом, метод ручного пополнения онтологии в отдельных случаях может быть весьма надёжным и точным. Преимуществом данного метода является то, что пополнение онтологии вручную может произойти в относительно короткие сроки. Однако в рамках данной задачи этот метод не подходит, поскольку онтологий может быть несколько и поддержка вручную влечёт за собой денежные затраты на экспертов предметной области и инженеров онтологии. Отдельным недостатком является то, что в данном случае пополнение вручную может занять много времени.  1.3.2. Интеграция двух онтологий К более сложным методам расширения онтологии можно отнести интеграцию двух онтологий. Для одной и той же предметной области существует множество различных онтологий. Созданные разными командами инженеров одной предметной области с разными целями данные онтологии могут иметь схожие сущности и связи и, в то же время, могут сильно отличаться друг от друга отдельными понятиями или синонимами. Это связано с тем, что при создании онтологии во внимание могли приниматься различные факторы, экспертами предметной области могли быть вынесены различные решения о связях между словами, о глубине и ширине онтологии, о необходимости использования определённых синонимов в зависимости от контекста и т.д. Интеграция двух онтологий друг с другом может значительно улучшить качество онтологии, но при интеграции возможны конфликты понятий и связей. Данные конфликты решает эксперт предметной области. В одной из статей [12], посвящённой методу интеграции онтологий, первым этапом идёт построение базовой онтологии, которая должна быть выполнена качественно, чтобы остальные этапы интеграции прошли корректно. Дальше идёт этап разрешения отдельных лексических конфликтов онтологий. Конфликты могут содержаться в наименовании объектов и атрибутов. Для этого применяют один из множества уже существующих алгоритмов нахождения орфографических сходств. Следующим этапом является нахождение семантической схожести объектов. Авторы статьи применяют единую онтологическую модель [12], которая подразумевает изучение связей объекта в онтологии и изучение связей объекта в ресурсе. Существует формула, высчитывающая семантическую близость понятий на основе семантической близости онтологий, контекстных метаданных объектов онтологии, а также функции учёта коэффициентов важности утверждений. Для точности авторы добавляют в формулу меру близости. Вычислив взвешенную сумму семантических связей понятий интегрируемой и базовой онтологий, можно судить о близости двух концептов. Полученные результаты предоставляются эксперту предметной области, который выносит решение о близости понятий и о возможном их слиянии друг с другом. Следующий этап – интеграция близких понятий онтологии, которые подтвердил или указал эксперт предметной области. Интеграцию можно произвести, используя функцию дополнения [12]. В результате будет получена расширенная базовая модель предметной области. Эффективность данного метода была подтверждена автором статьи на основе реальных данных Вологодского комбината хлебопродуктов. Использование указанного метода позволило в более короткие сроки сформировать сводную отчётность предприятия. Преимуществом данного метода является автоматическое вычисление семантической близости двух понятий, позволяющее заметно сократить время анализа интегрируемой и базовой онтологии. Также автоматизирована сама интеграция двух онтологий. Однако функция оценки корректности вычисленных семантических связей требует участия эксперта предметной области. Недостатком такого метода расширения онтологии является необходимость привлечения эксперта предметной области, поскольку велика вероятность возникновения конфликтов в ходе вычисления семантической близости понятий, а также велика вероятность возникновения ошибки. К сожалению, если предоставить непрофессиональному пользователю возникшие конфликты, ему может не хватить квалификации в разрешении данных конфликтов, появившихся при использовании данного метода. Важно рассмотреть этот метод в отношении к поставленной в данной работе проблеме. Как уже было указано в предыдущем разделе, в данной системе планируется использование онтологий различных предметных областей. Пользователю будет несподручно искать онтологии необходимых ему предметных областей.  Как вариант, в данном случае можно привлечь эксперта предметной области, для чего необходимы финансовые затраты. Также возникает проблема перерывов между работой эксперта предметной области, что может привести к неактуальному состоянию онтологии. Отдельной проблемой является поиск актуальной интегрируемой онтологии. Весьма велика вероятность, что интегрируемая онтология не будет содержать в себе необходимых сущностей для поддержания актуальности текущей онтологии в системе, в результате чего интеграция двух онтологий будет бессмысленна. Таким образом, метод интеграции двух онтологий может обеспечить хорошее расширение выбранной онтологии предметной области, если интегрируемая онтология является качественной и актуальной. Проблема заключается в поиске качественной онтологии. Более того, сложно найти онтологию, которая будет своевременно обновляться в соответствии с обновлением данных о предметной области в сети Интернет. Также необходимо привлечение эксперта предметной области. Именно по этим причинам данный метод не вполне подходит для решения поставленной в данной работе задачи. 1.3.3. Автоматическое пополнение онтологии Наиболее интересный и технически непростой метод расширения онтологии – автоматическое расширение онтологии. Онтология обновляется алгоритмом на основе текстового корпуса. Слова в текстах индексируются, в результате каждое слово имеет свой вес. Если слово с большим весом не найдено в онтологии, алгоритм вычисляет, с каким понятием в онтологии можно связать данное слово и какую связь установить. В настоящее время существует множество различных алгоритмов автоматического расширения онтологии. Один из наиболее популярных методов заключается в использовании ассоциативных правил [11]. Ассоциативные правила нужны для обнаружения закономерности между двумя событиями. Например, «если произошло событие X, то произойдёт событие Y». В алгоритме пополнения онтологии ассоциативные правила позволяют выставить связи найденных в тексте слов, а также связать их, в свою очередь, с понятиями в онтологии. Данный метод также требует привлечения эксперта предметной области. В случае нахождения слова, которое необходимо добавить в онтологию, потребуется оценка корректного добавления слова и связи в онтологию. Сам алгоритм может указать неточную связь или неправильно посчитать вес слова. По ошибке, в онтологию может быть добавлено лишнее понятие или некорректная связь, в результате чего может быть понижено качество онтологии. Поэтому необходима оценка добавляемого слова и связи в онтологию. Эксперт предметной области однозначно может вынести решение о корректности алгоритма. Однако, в данной работе отсутствует финансирование, из-за чего привлечение эксперта предметной области исключено. Решением данной проблемы может послужить некий интерфейс для конечного пользователя, в котором сам пользователь может оценить добавление слова и его связи в онтологию предметной области. Возможно, качество онтологии будет понижено, но не так сильно, как если бы алгоритм сам же добавлял новые сущности в онтологию. Пользователь может исключить появление очевидных ошибок алгоритма. Таким образом, метод полностью автоматизирует пополнение онтологии. При определённых условиях метод не требует привлечение эксперта предметной области и точно не требует инженера онтологии, что исключено в данной работе, поскольку отсутствует финансирование. Также данный метод обеспечивает пополнение онтологии за счёт найденных в сети Интернет текстов, в которых зачастую поддерживается актуальность информации. Индексация данных текстов позволит принимать во внимание слова или фразы, которые отсутствуют в онтологии, однако имеют достаточно высокий вес. 1.3.4. Сравнение методов расширения онтологии В ходе изучения статей по методам расширения онтологии были проанализированы преимущества и недостатки трёх основных методов расширения онтологии. Ниже представлена таблица сравнения выбранных методов пополнения онтологии в контексте поставленной проблемы в данной работе. Таблица 1.1. Сравнение методов расширения онтологии  Ручное расширение онтологии Интеграция онтологий Автоматическое пополнение онтологии Необходимость эксперта предметной области Высокая необходимость Высокая необходимость Низкая необходимость Необходимость инженера онтологии Высокая необходимость Низкая необходимость Нет необходимости Время обновления онтологии Зависит от времени работы эксперта предметной области и инженера онтологии Зависит от времени работы эксперта предметной области и инженера онтологии После каждой индексации текста Трудоёмкость при наличии нескольких онтологий разных предметных областей Очень высокая трудоёмкость Очень высокая трудоёмкость Очень низкая трудоёмкость Возможность реализации метода для разработанной системы Возможно, но необходимо финансирование Возможно, но необходимо финансирование Возможно Трудоёмкость реализации в рамках разработанной системы Низкая трудоёмкость Средняя трудоёмкость Высокая трудоёмкость  Как можно видеть из таблицы выше, для поставленной в работе проблемы с имеющимися ограничениями наиболее подходит метод автоматического расширения онтологии.  Главным преимуществом данного метода является то, что он обеспечивает пополнение онтологии без привлечения эксперта предметной области и инженера онтологии. Также у данного метода время обновления онтологии происходит каждый раз при запуске парсинга данных, что обеспечивает поддержание актуальности онтологии. Однако наибольшая сложность заключается в разработке алгоритма: это весьма трудоёмкий процесс, требующий знаний в индексации текста, разработке сложных алгоритмов и работе с онтологией. Таким образом, в данной работе для пополнения онтологии наиболее подходит алгоритм автоматического пополнения онтологии. 1.4. Форматы представления онтологии В настоящее время для работы с онтологиями существуют различные онтологические редакторы, которые позволяют наглядно создавать, расширять онтологии, создавать SPARQL-запросы и прочее. В предыдущей работе использовался один из наиболее популярных редакторов – Protégé. Онтология была экспортирована из онтологического редактора в формате turtle, поскольку используемая библиотека SPARQL.js [23] для создания запросов к онтологии обеспечивала корректную работу именно с этим форматом представления онтологии. Ниже в данном разделе будут подробно рассмотрены различные форматы представления онтологии и обоснован выбор формата turtle. Для начала необходимо понимать, что такое RDF-граф. RDF-граф описывает семантику через субъект, объект и предикат. Предикат отражает отношение субъекта и объекта. Через предикаты также обозначаются атрибуты субъекта. Различные форматы онтологии описывают предметную область через RDF-графы. По синтаксису формата визуализаторы онтологии отражают онтологию и обеспечивают работу с ней. Ниже рассмотрены следующие форматы представления онтологии: RDF, OWL, turtle. Данные форматы выбраны как наиболее популярные форматы представления. Онтологию в программе Protégé можно экспортировать во все три описанные формата и ещё ряд других. В данной работе разобраны только три формата. Примеры взяты из онтологии предметной области «Разлив нефти», которая интегрирована в разрабатываемую систему для структуризации данных. RDF – стандартный формат представления RDF-графа. В RDF для связи субъекта и объекта используются ссылки. Первая версия данного формата была опубликована W3C в 1998 году [26]. Пример триплета представлен ниже: <owl:Class rdf:about=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Разлив\">         <rdfs:subClassOf rdf:resource=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Техногенная_угроза\"/>     </owl:Class>  В данном примере субъект «Разлив» имеет свою ссылку, как и объект «Техногенная_угроза». Связаны они предикатом «subClassOf». Формат представления OWL также базируется на ссылках. Первая версия данного формата была опубликована W3C в 2004 году [26]. Ниже представлен субъект «Разлив» и его связи с другими объектами, например, предикат «содержит_вещество» с объектом «Нефтепродукт». <owl:NamedIndividual rdf:about=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Разлив\">         <rdf:type rdf:resource=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Разлив\"/>         <rdf:type rdf:resource=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Экология\"/>         <содержит_вещество rdf:resource=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Нефтепродукт\"/>         <содержит_вещество rdf:resource=\"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#Нефть\"/>     </owl:NamedIndividual> Описания RDF и OWL весьма схожи между собой и достаточно громоздкие из-за ссылок. Это вредит наглядности представления онтологии, хотя это и не является большой необходимостью, поскольку редки случаи, когда работа с онтологией ведётся в каком-либо формате без использования программ визуализации онтологии. Формат представления turtle рекомендован для представления RDF-графа консорциумом всемирной паутины. Первая версия данного формата была опубликована W3C в 2008 году [26]. Триплет в формате turtle выглядит следующим образом: :Разлив rdf:type owl:Class ;         rdfs:subClassOf :Техногенная_угроза . В формате turtle все субъекты наследуются от общего класса Class. В примере выше субъект «Разлив» является также подклассом объекта «Тенхногенная_угроза». Если у одного субъекта несколько пар «предикат – объект», эти данные указываются через символ «;», а конец описания субъекта – через символ «.». В примере ниже указывается, что у субъекта «Разлив» присутствует «Нефть», «Нефтепродукт»: :Разлив :произошло_по_вине :Energy_Transfer_Partners ;         :содержит_вещество :Нефтепродукт ,                      :Нефть . Формат представления turtle достаточно прост в понимании и позволяет наглядно отразить сущности онтологии и их отношения. Все три формата представления онтологии имеют чёткую, понятную структуру. Не так уж важна наглядность формата представления онтологии, поскольку нет необходимости работать с онтологией вручную. Однако в данной работе существует ограничение. Для создания SPARQL-запросов к онтологии в разработанной системе используется библиотека SPARQL.js, которая поддерживает работу с онтологией в формате .ttl. Есть возможность выбора новой библиотеки, которая позволит создавать SPARQL-запросы к онтологии в любом из указанных выше форматах, однако необходимости в этом нет. Также необязательно для реализации алгоритма использовать какой-либо конкретный формат представления онтологии. Таким образом, было решено оставить формат представления turtle для дальнейшей работы с онтологией.  1.5. Выводы В анализе статей, посвящённых различным методам расширения онтологии, были разобраны метод ручного пополнения онтологии, интеграция двух онтологий и автоматическое пополнение онтологии. В результате были выделены основные преимущества и недостатки методов по отношению к поставленной проблеме. Только автоматическое пополнение онтологии позволит поддерживать актуальное состояние онтологии без привлечения эксперта предметной области и инженера онтологии. Также в данной главе были разобраны различные форматы представления онтологии. Формат turtle был выбран для работы с онтологией, поскольку представляет подробное и удобное для понимания описание онтологии, а также используемая для SPARQL-запросов библиотека обеспечивает корректную работу с онтологией в данном формате. Таким образом, было решено реализовывать алгоритм автоматического расширения онтологии. Онтология будет представлена в формате turtle, как это было в предыдущей работе. В следующей главе подробно описано проектирование выбранного алгоритма пополнения онтологии. Глава 2. Проектирование сервиса Для разработки алгоритма расширения онтологии необходимо детально разобрать выбранный алгоритм для реализации в разработанной системе мониторинга глобальных процессов. В данной главе описано проектирование сервиса с использованием диаграммы нотации «Процесс» и метаязыка для описания работы алгоритма. Также подробно разобраны источники данных для извлечения и анализа текстов.  2.1. Проектирование алгоритма В предыдущей главе был обоснован выбор автоматического пополнения онтологии для системы мониторинга глобальных процессов. Для описания работы алгоритма в контексте разработанной системы была составлена диаграмма нотации «Процесс» (см. рис. 2.1.).  Рисунок 2.1. Диаграмма нотации «Процесс»  В разработанной программе система ведёт поиск новостей по заданному пользователем запросу. Найденные тексты индексируются. Предполагается, что будет выбрано значение веса для извлечения наиболее значимых понятий. По заданному значению веса отбираются наиболее значимые понятия. Поиск этих понятий ведётся в онтологии. При отсутствии значимого понятия в онтологии это понятие добавляется в онтологию. Необходимо разобрать инструменты, позволяющие производить указанные операции индексации текста и поиска понятий в онтологии. Результаты анализа существующих инструментов представлено в данной работе в третьей главе. Для непосредственной реализации был составлен также псевдокод, описывающий алгоритм. public valuableIndex: Number  func index(texts: Array)  indexes: Dictionary  valuableTexts: Array  for text in texts   indexes = getIndexes(for: text)   indexedTexts = indexes.allKeys   for indexedText in indexedTexts    index = indexedTexts[indexedText]    if index >= valuableIndex     valuableTexts.add(indexedText)  news: Array news = searchNews(by query:userQuery) mostValuable: Array mostValuable = index(texts: news) for valuableText in mostValuable  if Ontology.exist(valuableText) == false   Ontology.add(valuableText) В данном псевдокоде указана переменная valuableIndex, которая будет хранить в себе вес слова, который решено будет принять за вес значимого слова. Функция index(texts:) индексирует тексты и выбирает слова с наибольшим весом. После этого программа перебирает каждое проиндексированное слово с большим весом и проверяет его наличие в онтологии. В случае, если в онтологии отсутствует значимое понятие, оно добавляется в онтологию.  2.2. Источники данных В работе предыдущего года извлечение данных велось из поискового источника Google для презентации работы системы. Предполагается, что в будущем система позволит извлекать новости также из новостных источников Яндекс и Mail. Поскольку разные поисковые источники используют разные поисковые алгоритмы для выборки новостей, извлечение данных из источников Яндекс и Mail обеспечит разнообразие результатов. На момент написания данной работы разработанная система обеспечивает извлечение данных с первой страницы новостного раздела поискового источника Google. Для реализации алгоритма предполагается использование тех же данных, что извлекались из алгоритма при парсинге новостей в работе прошлого года. Однако на момент разработки системы основным преимуществом новостной страницы Google являлась наиболее полная аннотация, которая предоставляла больше информации для извлечения и структуризации данных. К сожалению, с тех пор Google пересмотрели допустимый объём аннотации и сократили его. В результате количество информации по одной новости значительно уменьшилось (см. рис.2.2).  Рисунок 2.2. Результат поиска новостной страницы Google Из-за малого количества информации может возникнуть риск нехватки данных для индексации текста и поиска семантических связей. В связи с этим необходимо пересмотреть возможные варианты извлечения данных. Был проведён повторный анализ ресурсов Яндекс и Mail. Содержание аннотации в обеих системах весьма краткий и не подходит для анализа текста. Пример новостной страницы Яндекса приведён на рисунке 2.3.  Рисунок 2.3. Результат поиска новостной страницы Яндекс Поскольку новостные источники популярных поисковых систем не предоставляют достаточной информации для индексации текста, был проведён анализ новостных источников, не привязанных к поисковым системам. Новостные разделы поисковых систем были выбраны для разрабатываемой системы, поскольку позволяли вести поиск новостей различных источников по пользовательскому запросу. Поскольку новостные разделы не подходят для анализа текста, было решено искать прочие новостные ресурсы, не привязанные к поисковым системам. Однако в таком случае главным критерием выбора новостного ресурса является наличие поисковой строки для ввода пользовательского запроса. Были проанализированы различные новостные ресурсы, такие как Forbes [15], REGNUM [21] и другие. Во многих случаях в новостных ресурсах по запросу поиска возвращались аннотации либо с одним заголовком, либо с очень короткой аннотацией (см. рис.2.4). Предоставленные новостным ресурсом данные не подходили для анализа текста.  Рисунок 2.4. Результаты новостного ресурса Forbes по запросу «разлив нефти» Однако были найдены два источника: РБК [13] и Техноблог [14], которые предоставляли расширенную аннотацию найденной статьи. В обоих источниках присутствует поисковая строка для ввода запроса пользователя. В ходе анализа данных источников выяснилось, что РБК предоставляет информацию на различные темы, в то время как Техноблог акцентирует внимание на природных ресурсах и технологиях. Также было выяснено преимущество новостного ресурса РБК. Поиск в данном ресурсе ведётся не только со словами в той форме, в которой они упомянуты в запросе, но и по словам в разных падежах, что позволит предоставить пользователю более полные результаты. Поскольку в разрабатываемой системе в будущем планируется использование онтологий различных предметных областей, было решено использовать новостной ресурс РБК для большей оптимальности (см. рис.2.5).  Рисунок 2.5. Результаты новостного ресурса РБК по запросу «разлив нефти» Таким образом, было решено использовать информацию с новостного ресурса РБК для получения данных для анализа. Чтобы начать работать с данным ресурсом, необходимо внести незначительные правки в разрабатываемую систему. 2.3. Выводы В данной главе рассматривается построение алгоритма автоматического пополнения онтологии в контексте разрабатываемой системы. Подробно разобраны функции алгоритма, необходимые для реализации. Построенная диаграмма нотации «Процесс» описывает логику работы алгоритма. Структура алгоритма, необходимая для реализации, описана псевдокодом. Отдельно разобраны различные источники данных. Выяснилось, что новостной источник поисковой системы Google уже более не подходит для задач, указанных в данной работе. В результате был проведён повторный анализ новостных источников. Наиболее подробную информацию предоставляет сайт РБК, который решено было выбрать как новый источник информации. В результате был спроектирован алгоритм автоматического расширения онтологии, который необходимо реализовать в разрабатываемой системе. Также появилась необходимость изменить новостной источник с новостного раздела Google на сайт РБК для более расширенной информации по запросу пользователя. В следующей главе приводится описание разработки и внедрения данного алгоритма в существующую систему мониторинга глобальных процессов. Глава 3. Разработка автоматического пополнения онтологии В данной главе приводится подробный разбор инструментов, которые были использованы в ходе разработки алгоритма автоматического пополнения онтологии. Также описана разработка алгоритма с использованием выбранных инструментов разработки и проведено тестирование алгоритма. 3.1. Обоснование выбора инструментов разработки Для реализации системы мониторинга глобальных процессов в работе прошлого года были использованы следующие инструменты разработки:     1. Язык программирования JavaScript, который является одним из самых популярных языков программирования [9], что обеспечивает большое количество библиотек с готовыми решениями.     2. WebStorm IDE\\xa0[25], поскольку данная среда разработки поддерживает работу с JavaScript и Node.js [17, 19].     3. Библиотека NightMare [16], предоставляющая удобные функции для парсинга данных в сети Интернет.     4. Библиотека SPARQL.js [23], которая обеспечивает создание SPARQL-запросов к онтологии. Все указанные инструменты обеспечили реализацию функциональных возможностей по извлечению и анализу данных в Интернете. Поскольку алгоритм автоматического пополнения данных необходимо интегрировать в разрабатываемую систему, важно, чтобы инструменты не конфликтовали друг с другом. Для этого необходима реализация алгоритма на языке JavaScript в среде разработки WebStorm. Для реализации алгоритма необходимо решить две основные задачи: индексация текста и добавление понятия в онтологию, поскольку задача поиска понятия в онтологии была реализована ранее. Для получения ключевых слов из текста необходима библиотека, позволяющая работать со словами на русском языке. В ходе анализа существующих библиотек было найдено множество, написанных на языке Python. Однако, для использования библиотеки в существующем проекте, важным условием является возможность её использования на JavaScript. По данным критериям было найдено две библиотеки, позволяющие извлечь ключевые слова: retext-keywords [22] и node-rake [18]. Для выбора библиотеки была составлена таблица сравнения (см. табл. 3.1.). Таблица 3.1. Сравнение библиотек для извлечения ключевых слов  retext-keywords node-rake Извлечение ключевых слов Извлечение ключевого слова, фразы Извлечение ключевых фраз Рейтинг на сайте github.com 167 69 Работа на языке JavaScript Да Да Поддержка русского языка Да Да  Как видно из таблицы, библиотека retext-keywords имеет рейтинг выше, чем у библиотеки node-rake, а также у неё есть возможность извлекать ключевое слово, что является значительным преимуществом. Поэтому было решено использовать данную библиотеку для выделения ключевых слов в тексте. 3.2. Изменение источника данных В разрабатываемой системе для поиска новостей был использован новостной источник Google. В ходе исследования из-за изменения структуры новостного источника было решено сделать выбор в пользу новостного ресурса РБК. Поскольку Google не одобряли парсинг поисковой системы, что выяснилось в ходе разработки прошлого года, были использованы различные функции библиотеки Nightmare для имитации нажатия клавиш, для выбора новостного раздела и прочее. В данном случае нет необходимости вводить запрос в поисковую строку ресурса, можно осуществить поиск путём url-запроса, поскольку его структура проста: в запросе передаётся текст, указанный в поиске, с заменой пробелов знаком «+». Также необходимо учесть регион запроса: для поиска новостей по всему миру необходимо указывать домен rbc без привязки к региону. Таким образом, поисковый запрос будет выглядеть следующим образом: https://www.rbc.ru/search/?project=rbcnews&query= Для поиска новостей по запросу необходимо в конце этой строки приписать сам запрос с заменой пробелов знаком «+». Отдельно необходимо учесть используемую в системе структуру новости и выяснить, возможно ли придерживаться данной структуры с новостным источником РБК. В разрабатываемой системе прошлого года структура новости была следующей:     1. Заголовок новости.     2. Текст новости.     3. Дата новости.     4. Источник новости. Для изменения используемого источника данных была исследована структура новости в новом ресурсе. Ресурс РБК предоставляет заголовок статьи под тегом «search-item__title», текст статьи – «search-item__text». Три объекта данных: категория, дата и время публикации новости представлены в общем теге – «search-item__category». Как оказалось, в поисковой выдаче не предоставляется возможность получить источник новости, который использовался ранее в структуре новости. Поскольку источник новости не несёт в себе критически важной информации на данном этапе работы, было решено вести разработку без этих данных. Вместо этого было решено выделять категорию новости. Извлечь дату и категорию новости по отдельности нет возможности, однако формат представления данных в теге один и тот же: все три объекта приведены через запятую. Извлечение этих данных ведётся через разделение текста по запятой. Первый элемент – источник новости, второй – дата новости. Нет необходимости извлекать отдельно время публикации. Однако в ходе разработки стало известно, что в некоторых элементах поиска предоставляется информация без категории (см. рис.3.1.).  Рисунок 3.1. Элемент поиска без указания категории новости Поскольку дата и время публикации статьи обязательно будут присутствовать, при разработке был учтён случай отсутствия категории статьи: если в данном теге через запятую представлено только два элемента, категория отсутствует. Таким образом, в разрабатываемой системе было реализовано извлечение данных из нового новостного ресурса – РБК. 3.3. Реализация алгоритма автоматического пополнения онтологии После реализации извлечения данных из нового новостного ресурса необходимо реализовать также извлечение ключевых слов из текстов найденных новостей. В первом разделе данной главы было обосновано решение использования библиотеки retext-keywords для извлечения ключевых слов. Данная библиотека была установлена в проекте. Отдельно был создан класс KeywordsExtraction, принимающий на вход массив новостей и возвращающий массив ключевых слов. Алгоритм нахождения ключевых слов реализован следующим образом (см. рис.3.2).  Рисунок 3.2. Алгоритм нахождения ключевых слов Было решено считать часто встречающимися ключевыми словами те, которые встречались три раза и больше, так как в тексте большое количество слов встречались хотя бы два раза. В результате поиска были выделены ключевые слова. Однако также в выборку вошло большое количество незначимых слов, например, «в», «около», «или» и пр. Данные слова необходимо отнести к стоп-словам. Это было реализовано отдельно и при сортировке слов была добавлена также функция проверки на стоп-слово. Таким образом, по запросу «разлив нефти» данный алгоритм возвращал следующие ключевые слова: [\\'результате\\', \\'произошел\\', \\'разлив\\', \\'нефти\\', \\'месте\\', \\'тихорецк\\', \\'саратовской\\', \\'области\\', \\'нефтепроводе\\', \\'транснефти\\'] Исходный код алгоритма извлечения ключевых слов представлен в приложении Б. Далее необходимо выбрать только те ключевые слова, которые отсутствуют в онтологии. Алгоритм нахождения понятий в онтологии был реализован в работе прошлого года. Отдельно в классе Ontology был реализован метод нахождения слов, которые отсутствуют в онтологии. Для этого была использована библиотека для создания SPARQL-запросов, ранее добавленная в проект. Для каждого ключевого слова, найденного в текстах, был создан запрос на существование данного слова в онтологии. Если вернулся пустой результат, слово не найдено в онтологии.  Таким образом, для указанных выше ключевых слов алгоритм нахождения слов, отсутствующих в онтологии, вернул следующие результаты: [\\'Результате\\', \\'Произошел\\', \\'Нефти\\', \\'Месте\\', \\'Тихорецк\\', \\'Саратовской\\', \\'Области\\', \\'Нефтепроводе\\', \\'Транснефти\\'] Как можно видеть из результатов, ключевое слово «Разлив» было исключено из набора ключевых слов, поскольку оно уже добавлено в онтологию. Однако, данный результат нельзя считать окончательным, поскольку ключевые слова указаны не в начальной форме.  Заключение В работе была обоснована необходимость создания системы мониторинга глобальных процессов, были описаны предыдущие результаты разработки системы, на основе которых базируется данная работа. Отдельно была обоснована необходимость проектирования алгоритма расширения онтологии для разработанной системы. Были исследованы три метода расширения онтологии: пополнение онтологии вручную, интеграция онтологий, автоматическое расширение онтологии. В ходе анализа различных методов расширения онтологии был выбран метод автоматического расширения онтологии, позволяющий эффективно дополнять онтологию на основе данных из Интернет-источников. Отдельно были рассмотрены три основные формата представления онтологии: RDF, OWL и turtle. В результате анализа выяснилось, что все три формата представляют полную и наглядную информацию об онтологии. Однако, для разработки системы использовалась онтология в формате turtle, поскольку именно с таким форматом работает используемая библиотека для создания SPARQL-запросов в онтологии. Поэтому было решено оставить формат turtle для дальнейшей разработки системы. Было проведено проектирование данного алгоритма для последующей реализации его в разрабатываемой системе. Наглядное представление работы алгоритма описано диаграммой нотации «Процесс». Для реализации данного алгоритма в разрабатываемой системе была подробно описана структура посредством псевдокода. Для анализа текста был рассмотрен текущий источник данных для поиска информации по пользовательскому запросу. Выяснилось, что новостной раздел Google изменился и в настоящий момент предоставляет короткую аннотацию статьи, что не позволяет провести подробный анализ текста. В результате анализа источников данных было решено изменить используемый новостной источник и вести работу с сайтом РБК. Учитывая особенности разрабатываемой системы, отдельно были рассмотрены возможности интегрирования спроектированного алгоритма в разрабатываемую систему. Были изучены инструменты, позволяющие реализовать алгоритм автоматического пополнения онтологии. В данной работе используются язык программирования JavaScript и WebStorm IDE, поскольку разработка системы мониторинга глобальных процессов прошлого года велась с использованием данных инструментов. Для парсинга данных также используется библиотека NightMare, для SPARQL-запросов – библиотека SPARQL.js. Отдельно была внедрена библиотека для поиска ключевых слов в текстах – retext-keywords. Был разработан алгоритм поиска часто встречаемых ключевых слов в онтологии и проверка наличия данных слов в онтологии. Разработка алгоритма автоматического пополнения онтологии позволит поддерживать используемую онтологию в актуальном состоянии, благодаря чему система предоставляет пользователю более подробную информацию о процессе. Библиографический список     1. Вохминцева Т.В. Именование: нравственно-ценностные аспекты / Т.В. Вохнинцева, А.Е. Зимбули // Общество. Среда. Развитие (Terra Humana). 2014. № 33 (4). C. 135–138.     2. Ланин В.В. Мониторинг глобальных процессов на основе данных из интернет-новостей / В.В. Ланин, И.М. Шаляева, А.Ю. Скурихина. Пермь: Издательство Пермского государственного национального исследовательского университета, 2017. С. 67–70.     3. Липкин Ю.Г. Обзор современных поисковых систем: архитектура, инструменты поиска /Ю.Г. Липкин // Врач и информационные технологии. 2009. C. 40–44.     4. Шаляева И.М. Мониторинг экологических катастроф и их последствий на основе Internet-новостей / И.М. Шаляева. Таганрог: Издательство ЮФУ, 2016. С. 116–123.     5. Шаляева И.М. О проекте разработки системы мониторинга глобальных процессов на основе Интернет-новостей / И.М. Шаляева, В.В. Ланин, Л.Н. Лядова. Таганрог: Издательство ЮФУ, 2016. С. 166–170.     6. Hong J.L. Deep web data extraction / J.L. Hong // Systems Man and Cybernetics (SMC). 2010.     7. Shalyaeva I. Ontology-Driven System for Monitoring Global Processes on Basis of Internet News / I. Shalyaeva, V. Lanin, L. Lyadova // 11th IEEE International Conference on Application of Information and Communication Technologies (AICT), 2017. С. 385–389.     8. Simon-Nagy G. Ontology Extension for Personalized Accessible Indoor Navigation / G. Simon-Nagy, R. Fleiner // Springer International Publishing, 2018. С. 281-288.     9. StackOverflow Developer Survey Results [Электронный ресурс]. URL: https://insights.stackoverflow.com/survey/2018 (дата обращения: 03.03.2019).     10. Lanin V. Intelligent search and automatic document classification and cataloging based on ontology approach / V. Lanin, L. Lyadova // Information Theories & Applications. 2007. (14). C. 25–29.     11. Zhang D. Ontology Extension Based on Axiomatic Cognitive Model for Ontology Learning / D. Zhang // IEEE International Conference on Computer and Communications. 2016. С. 825-829.     12. Zhou Y. The Research of Concept Extraction in Ontology Extension Based on Extended Association Rules / Y. Zhou, L. Zhang, and S. Niu // Proceedings of ICOACS. 2016. С. 111-114.     13. РБК [Электронный ресурс]. URL: https://www.rbc.ru/ (дата обращения: 12.03.2019).     14. Техноблог [Электронный ресурс]. URL: https://teknoblog.ru/ (дата обращения: 12.03.2019).     15. Forbes [Электронный ресурс]. URL: https://www.forbes.ru/ (дата обращения: 12.03.2019).      16. Nighmare [Электронный ресурс]. URL: https://github.com/segmentio/nightmare (дата обращения: 11.03.2018).      17. Node.js [Электронный ресурс]. URL: https://nodejs.org/en/ (дата обращения: 10.03.2018).     18. Node-rake [Электронный ресурс]. URL: https://github.com/waseem18/node-rake/ (дата обращения: 25.03.2019).     19. NPM [Электронный ресурс]. URL: https://www.npmjs.com/ (дата обращения: 10.03.2018).     20. RapidMiner [Электронный ресурс]. URL: https://rapidminer.com/ (дата обращения: 10.02.2018).     21. REGNUM [Электронный ресурс]. URL: https://regnum.ru/ (дата обращения: 12.03.2019).     22. Retext-keywords [Электронный ресурс]. URL: https://github.com/retextjs/retext-keywords (дата обращения: 25.03.2019).     23. SPARQL.js [Электронный ресурс]. URL: https://github.com/RubenVerborgh/SPARQL.js/ (дата обращения: 11.03.2018).     24. Visual Studio [Электронный ресурс]. URL: https://www.visualstudio.com/ (дата обращения: 05.03.2018).     25. WebStorm [Электронный ресурс]. URL: https://www.jetbrains.com/webstorm/ (дата обращения: 05.03.2018).      26. World Wide Web Consortium (W3C) [Электронный ресурс]. URL: https://www.w3.org/ (дата обращения: 12.03.2019). Приложение А. Техническое задание        Проектирование средств автоматизации расширения онтологии на основе Интернет-источников  Техническое задание Листов 6                   1.1. Введение 1.1.1. Наименование программы Наименование – система мониторинга глобальных процессов: извлечение и структуризация данных из сети интернет. 1.1.2. Краткая характеристика области применения Программа предназначена для структуризации данных по новостному запросу. 1.2. Основания для разработки 1.2.1. Основания для проведения разработки Документ, на основании которого ведется разработка: заявление на написание выпускной квалификационной работы, подписанное деканатом ВШЭ. 1.2.2. Наименование и условное обозначение темы разработки Наименование: автоматизированный алгоритм пополнения онтологии. Условное обозначение: алгоритм пополнения онтологии. 1.3. Назначение разработки 1.3.1. Функциональное назначение Функциональным назначением является пополнение онтологии для поддержки актуального состояния онтологии при получении результатов запроса пользователя. 1.3.1. Эксплуатационное назначение Программа представляет собой web-клиент, доступ к которому может иметь любой пользователь. 1.4. Требования к программе 1.4.1. Требования к функциональным характеристикам 1.4.1.1. Требования к составу выполняемых функций Программа должна корректно выполнять следующие функции:     1. Извлечение данных из новостного источника поисковой системы Google по запросу пользователя.     2. Индексация текста для поиска наиболее важных понятий.     3. Пополнение онтологии наиболее важными понятиями, которые ранее отсутствовали в онтологии. 1.4.1.2. Требования к организации входных данных Входными данными для программы будет являться запрос пользователя типа строка. Запрос будет использоваться исключительно для извлечения данных, после чего удаляется из памяти. 1.4.1.3. Требования к организации выходных данных Выходными данными является обновлённая онтология в соответствии с данными из новостного источника. 1.4.1.3. Требования к временным характеристикам Требования к временным характеристикам программы не предъявляются. 1.4.2. Требования к надежности 1.4.2.1. Требования к обеспечению надежного (устойчивого) функционирования программы Надежное (устойчивое) функционирование программы обеспечено наличием подключения к сети Интернет. 1.4.2.2. Время восстановления после отказа При отказе программы ввиду отсутствия подключения к сети Интернет программа успешно восстановится при подключении к сети Интернет.   1.4.2.3. Отказы из-за некорректных действий оператора Программа предусматривает последовательность действий оператора, способных привести к отказу программы, и корректно обрабатывает их или же не допускает возможности выполнения этих действий. 1.4.3. Условия эксплуатации 1.4.3.1. Климатические условия эксплуатации Климатические условия эксплуатации должны удовлетворять требованиям к климатическим условиям эксплуатации программного оборудования, с помощью которого будет вестись работа с программой. 1.4.3.2. Требования к видам обслуживания Для доступа пользователя к программе необходимо приобрести домен и хостинг для программы и своевременно оплачивать их для обеспечения доступа к программе. 1.4.3.3. Требования к численности и квалификации персонала Минимальное количество персонала для работы с программой – две штатные единицы. Администратор, обеспечивающий доступ к программе путём своевременной оплаты хостинга и домена, и конечный пользователь. 1.4.4. Требования к составу и параметрам технических средств В состав технических средств должен входить программный компьютер с доступом к сети Интернет и устройствами для ввода запроса. 1.4.5. Требования к информационной и программной совместимости 1.4.5.1. Требования к информационной структуре и методам решения На входе и выходе информация должна быть представлена в типе строка. 1.4.5.2. Требования к исходным кодам и языкам программирования Требования к исходным кодам и языкам программирования не предъявляются. 1.4.5.3. Требования к программным средствам, используемым программой Требования к программным средствам, используемым программой, не предъявляются. 1.4.5.4. Требования к защите информации и программ Требования к защите информации и программ не предъявляются. 1.4.6. Требования к маркировке и упаковке Программа поставляется в качестве web-клиента, доступного в сети Интернет. 1.4.7. Требования к транспортированию и хранению Требования к транспортированию и хранению не предъявляются, так как программа доступна в сети Интернет. 1.5. Требования к программной документации 1.5.1. Предварительный состав программной документации Состав программной документации должен включать в себя:     1. Техническое задание.     2. Исходный код программы. 1.6. Технико-экономические показатели Ориентировочные экономические показатели не высчитываются. Предполагаемое число использования программы одним пользователем не ограничено. 1.7. Стадии и этапы разработки 1.7.1. Стадии разработки Разработка должна быть проведена в стадии:     1. Разработка технического задания.     2. Анализ методов и средств разработки алгоритма.     3. Проектирование алгоритма.     4. Разработка алгоритма.     5. Интеграция алгоритма в существующую систему. 1.8. Порядок контроля и приёмки 1.8.1. Виды испытаний После разработки программы необходимо провести различные испытания, подтверждающие корректную работу программы. 1.8.2. Общие требования к приемке работы Общие требования к приемке работы заключаются в наличии корректно работающей программы в соответствии с поставленными требованиями. Приложение Б. Исходный код алгоритма поиска  ключевых слов в тексте const retext = require(\\'retext\\'); const keywords = require(\\'retext-keywords\\'); const toString = require(\\'nlcst-to-string\\');  class KeywordExtraction {     async getKeywords(arr) {         let results = [];          for (let news of arr) {             const text = news.text;              retext()                 .use(keywords)                 .process(text, (err, file) => {                     results = results.concat(this.done(err, file));                 });         }          return this.getFrequentKeywords(results);     }      done(err, file) {         if (err) throw err;          const keywords = file.data.keywords.map((keyword) => toString(keyword.matches[0].node).toLowerCase());          return keywords;     }      getFrequentKeywords(arr) {         const frequentTimes = 3;          const stopWords = [\"по\", \"на\", \"в\", \"что\", \"не\", \"и\", \"около\"];          const importantWords = arr.filter((word) => {            const someWords = arr.filter((someWord) => someWord === word);            const isImportantWord = someWords.length >= frequentTimes && !stopWords.includes(word) && word.length > 1;             return isImportantWord;         });          const uniqImportantWords = new Set();          importantWords.forEach((word) => uniqImportantWords.add(word));          return [...uniqImportantWords];     } }  module.exports = KeywordExtraction; Приложение В. Исходный код поиска отсутствующих в онтологии ключевых слов const SPARQL = require(\"sparqljs\"); const Generator = SPARQL.Generator; const rdfstore = require(\\'rdfstore\\'); const ldf = require(\\'ldf-client\\'); const Path = require(\"path\"); const fs = require(\"fs\");  class Ontology {      constructor() {         this.ontologyPath = \"http://www.semanticweb.org/3453453567/ontologies/2017/4/untitled-ontology-73#\"     }      async checkAndAddKeywords(arr) {         const results = [];          for (let word of arr) {             word = this.capitalizeFirstLetter(word);             const result = await this.query(word);             if (!result.length) {                 results.push(word);             }         }          return results;     }  capitalizeFirstLetter(string) {     return string[0].toUpperCase() + string.slice(1) }  async query(word) {     try {         const store = await this.createStore();          const path = \"/../rdf.ttl\";         const syncPath = Path.join(__dirname, path);          const rdf = fs.readFileSync(syncPath).toString();          await this.storeLoadRdf(store, rdf);          //PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>         //?subject ?object          const query = `PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>                     SELECT ?entity                     WHERE { ?entity rdf:type <${this.ontologyPath}${word}> }`;//WHERE { ?subject a <${this.ontologyPath}${word}> }`;          const result = await this.storeExecute(store, query);          store.close();          return result     }     catch (error) {         throw error     } };  storeExecute(store, query) {         return new Promise((resolve, reject) => {             store.execute(query, (error, result) => {                 if (error) {                     return reject(error);                 }                  return resolve(result);             });         });     } }  module.exports = Ontology;  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bj6QVGZFn80",
        "outputId": "3f45d228-5125-4abb-e2d1-18740837c856"
      },
      "source": [
        "with open(\"/content/ЛядоваЛюдмилаНиколаевна.data\", \"rb\") as f:\n",
        "    arr = pickle.load(f)\n",
        "arr.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1285, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMZR2uJVPlpL",
        "outputId": "c5a9fc59-036b-4fcb-e6a0-9ca35588029c"
      },
      "source": [
        "%time embeddings = modelSum.run_embeddings(text, num_sentences=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 9s, sys: 2.54 s, total: 1min 12s\n",
            "Wall time: 1min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOOWrfuCQQjm",
        "outputId": "06bee643-5f08-4411-bf0e-95466bccd328"
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tagdmZH_RC4x",
        "outputId": "6f3f311f-81e1-4eb9-8a83-d879a358588b"
      },
      "source": [
        "embeddings[0][100]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5039678"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRtLmFqmQRzm",
        "outputId": "b462f3d7-6a38-48c0-a855-8447568d3acb"
      },
      "source": [
        "sum = 0\n",
        "avg = 0\n",
        "mae = 0\n",
        "rmae = 0\n",
        "mse = 0\n",
        "rmse = 0\n",
        "for i in tqdm(range(11)):\n",
        "    for arrRow in range(len(arr)):\n",
        "        for j in range(768):\n",
        "            sum += embeddings[i][j] - arr[arrRow][j]\n",
        "            avg += 1\n",
        "            mae += abs(embeddings[i][j] - arr[arrRow][j])\n",
        "            rmae += abs(embeddings[i][j] - arr[arrRow][j])**0.5\n",
        "            mse += (embeddings[i][j] - arr[arrRow][j])**2\n",
        "            rmse += ((embeddings[i][j] - arr[arrRow][j])**2)**0.5\n",
        "print(f\"Count = {avg}\")\n",
        "print(f\"Sum = {sum}\")\n",
        "print(f\"Avg = {sum/avg}\")\n",
        "print(f\"MAE = {mae/avg}\")\n",
        "print(f\"RMAE = {rmae/avg}\")\n",
        "print(f\"MSE = {mse/avg}\")\n",
        "print(f\"RMSE = {rmse/avg}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [02:53<00:00, 15.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Count = 10855680\n",
            "Sum = 861.9069203189574\n",
            "Avg = 7.939686139596574e-05\n",
            "MAE = 0.42284764881514914\n",
            "RMAE = 0.595303441929663\n",
            "MSE = 0.30424415837365826\n",
            "RMSE = 0.42284764881514914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZqxdk6KRZ_9",
        "outputId": "f8435557-f533-4f50-ab90-dda91c1bbcc4"
      },
      "source": [
        "len(arr)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}