{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mysterious-humor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Using cached python-docx-0.8.10.tar.gz (5.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: lxml>=2.3.2 in /home/samoed/.local/lib/python3.8/site-packages (from python-docx) (4.6.3)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.10-py3-none-any.whl size=184489 sha256=926e5e4fd525243a20e0ee67deaffecace72f244ddf6883d5403c45508b7d72b\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/97/4c/2e/68066cbf12b9b2e66403da8982aaf4f656d9f5cb5dc3179e82\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.10\n",
      "Collecting textract\n",
      "  Downloading textract-1.6.3-py3-none-any.whl (21 kB)\n",
      "Collecting xlrd==1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting EbookLib==0.17.1\n",
      "  Downloading EbookLib-0.17.1.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting extract-msg==0.23.1\n",
      "  Downloading extract_msg-0.23.1-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 116 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
      "  Downloading beautifulsoup4-4.8.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting python-pptx==0.6.18\n",
      "  Downloading python-pptx-0.6.18.tar.gz (8.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.9 MB 18 kB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet==3.0.4 in /usr/lib/python3/dist-packages (from textract) (3.0.4)\n",
      "Collecting pdfminer.six==20181108\n",
      "  Downloading pdfminer.six-20181108-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docx2txt==0.8\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Collecting argcomplete==1.10.0\n",
      "  Downloading argcomplete-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting SpeechRecognition==3.8.1\n",
      "  Using cached SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: lxml in /home/samoed/.local/lib/python3.8/site-packages (from EbookLib==0.17.1->textract) (4.6.3)\n",
      "Collecting tzlocal==1.5.1\n",
      "  Downloading tzlocal-1.5.1.tar.gz (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: olefile==0.46 in /usr/lib/python3/dist-packages (from extract-msg==0.23.1->textract) (0.46)\n",
      "Collecting imapclient==2.1.0\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 611 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: soupsieve>=1.2 in /home/samoed/.local/lib/python3.8/site-packages (from beautifulsoup4==4.8.0->textract) (2.1)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=3.3.2 in /usr/lib/python3/dist-packages (from python-pptx==0.6.18->textract) (7.0.0)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-1.4.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 3.8 MB/s eta 0:00:01     |███████▎                        | 440 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /usr/lib/python3/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2019.3)\n",
      "Building wheels for collected packages: EbookLib, python-pptx, docx2txt, tzlocal\n",
      "  Building wheel for EbookLib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for EbookLib: filename=EbookLib-0.17.1-py3-none-any.whl size=38164 sha256=c68b01c39ba4659b9f932285d6807319afbc6956630481371f3d71c929d5f0f7\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/b4/eb/66/00c65b5bbf31ec34329090ec9fe8c8a8d9cb7a3a3d93841386\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.18-py3-none-any.whl size=275703 sha256=ede9047c80e2c122308af2a28d20e8e6e8df5032ef5ed344b1022bf5acaa53f6\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/11/2b/97/d82ca57932fa62d52c723024419c5ec3b7c0f7ecf0a0f06332\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3963 sha256=43dda59c18123894677a420fb8c886cdbc8aee5af82188d1f75f1457168b230b\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n",
      "  Building wheel for tzlocal (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tzlocal: filename=tzlocal-1.5.1-py3-none-any.whl size=17543 sha256=f2aee1cc0f1827d48f56370e35a5277f12acccdefa1240260e337b342756746f\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/3a/14/ce/9c504116f6b89e4a05ce0bc0f41983df280d7e00f463481900\n",
      "Successfully built EbookLib python-pptx docx2txt tzlocal\n",
      "Installing collected packages: xlrd, six, EbookLib, tzlocal, imapclient, extract-msg, beautifulsoup4, XlsxWriter, python-pptx, pycryptodome, sortedcontainers, pdfminer.six, docx2txt, argcomplete, SpeechRecognition, textract\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.3\n",
      "    Uninstalling beautifulsoup4-4.9.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.3\n",
      "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.4.0 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 pdfminer.six-20181108 pycryptodome-3.10.1 python-pptx-0.6.18 six-1.12.0 sortedcontainers-2.3.0 textract-1.6.3 tzlocal-1.5.1 xlrd-1.2.0\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already up-to-date: bert-extractive-summarizer in /home/samoed/.local/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/samoed/.local/lib/python3.8/site-packages (from bert-extractive-summarizer) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: transformers in /home/samoed/.local/lib/python3.8/site-packages (from bert-extractive-summarizer) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: spacy in /home/samoed/.local/lib/python3.8/site-packages (from bert-extractive-summarizer) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (2021.3.17)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers->bert-extractive-summarizer) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (20.8)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (4.56.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (8.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from spacy->bert-extractive-summarizer) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.1 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.7.3)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /home/samoed/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (3.0.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.4.2\n",
      "    Uninstalling transformers-4.4.2:\n",
      "      Successfully uninstalled transformers-4.4.2\n",
      "Successfully installed transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U python-docx\n",
    "!pip3 install -U textract\n",
    "!apt-get install antiword\n",
    "!pip3 install -U bert-extractive-summarizer\n",
    "!pip3 install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peewee import *\n",
    "\n",
    "database = SqliteDatabase('bd.db')\n",
    "\n",
    "class BaseModel(Model):\n",
    "    class Meta:\n",
    "        database = database\n",
    "\n",
    "class Campus(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    campus_name = TextField(column_name='Campus_name', null=True, unique=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Campus'\n",
    "\n",
    "class Cathedra(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    cathedra_name = TextField(column_name='Cathedra_Name', null=True, unique=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Cathedra'\n",
    "\n",
    "class Faculties(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    name_faculties = TextField(column_name='Name_Faculties', null=True, unique=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Faculties'\n",
    "\n",
    "class EducationalProgram(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    name_educational_program = TextField(column_name='Name_EducationalProgram', null=True, unique=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'EducationalProgram'\n",
    "\n",
    "class Position(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    position_name = TextField(column_name='PositionName', null=True, unique=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Position'\n",
    "\n",
    "class Professors(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    prof_name = TextField(column_name='Prof_Name', null=True)\n",
    "    profile_link = TextField(column_name='Profile_Link', null=True)\n",
    "    cathedra = ForeignKeyField(column_name='Cathedra_Id', model=Cathedra, null=True)\n",
    "    position = ForeignKeyField(column_name='Position_Id', model=Position, null=True)\n",
    "    competence = TextField(column_name='Competence', null=True)\n",
    "    embeddings = TextField(column_name='Embeddings', null=True)\n",
    "    \n",
    "    class Meta:\n",
    "        table_name = 'Professors'\n",
    "\n",
    "class Vkr(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    vkr_name = TextField(column_name='VKR_Name', null=True)\n",
    "    vkr_link = TextField(column_name='VKR_Link', null=True)\n",
    "    student = TextField(column_name='Student', null=True)\n",
    "    prof = ForeignKeyField(column_name='Prof_Id', model=Professors, null=True)\n",
    "    campus = ForeignKeyField(column_name='Campus_Id', model=Campus, null=True)\n",
    "    educational_program = ForeignKeyField(column_name='EducationalProgram_Id', model=EducationalProgram, null=True)\n",
    "    vkr_text_link = TextField(column_name='VKRText_Link', null=True)\n",
    "    \n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'VKR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "capable-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTables():\n",
    "    Campus.create_table()\n",
    "    Cathedra.create_table()\n",
    "    Faculties.create_table()\n",
    "    EducationalProgram.create_table()\n",
    "    Position.create_table()\n",
    "    Professors.create_table()\n",
    "    Vkr.create_table()\n",
    "    \n",
    "CreateTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compliant-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "from docx import Document\n",
    "import textract\n",
    "import magic\n",
    "import pickle\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "from collections import Counter\n",
    "import string\n",
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secret-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "#get links on letters for people\n",
    "r = requests.get('https://www.hse.ru/org/persons/?udept=135213')\n",
    "page = BeautifulSoup(r.text, 'html.parser')\n",
    "url = 'https://www.hse.ru'\n",
    "letters = []\n",
    "for link in page.findAll(\"div\", {'class': \"abc-filter__letter\"}):\n",
    "    if (str(link.contents)).find('\"') != -1:\n",
    "            s = url + '/org/persons/' + str(link.contents)[10:34]\n",
    "            letters.append(str(s))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "russian-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:46<00:00,  1.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Абашева Александра Сергеевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/485801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Авраменко Иван Александрович</td>\n",
       "      <td>https://www.hse.ru/org/persons/99247481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Агаркова Наталия Владиславовна</td>\n",
       "      <td>https://www.hse.ru/org/persons/28123133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Айхбергер Юрген Томас Германн</td>\n",
       "      <td>https://www.hse.ru/org/persons/359359486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Алексеева Лариса Николаевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/66770114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Аленина Карина Анатольевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/202287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Алова Надежда Владимировна</td>\n",
       "      <td>https://www.hse.ru/org/persons/28123313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Алферова Татьяна Викторовна</td>\n",
       "      <td>https://www.hse.ru/org/persons/359862051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Андреева Ольга Юрьевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/152893083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Андрианов Игорь Владимирович</td>\n",
       "      <td>https://www.hse.ru/org/persons/223725475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name                                      link\n",
       "0    Абашева Александра Сергеевна     https://www.hse.ru/org/persons/485801\n",
       "1    Авраменко Иван Александрович   https://www.hse.ru/org/persons/99247481\n",
       "2  Агаркова Наталия Владиславовна   https://www.hse.ru/org/persons/28123133\n",
       "3   Айхбергер Юрген Томас Германн  https://www.hse.ru/org/persons/359359486\n",
       "4     Алексеева Лариса Николаевна   https://www.hse.ru/org/persons/66770114\n",
       "5      Аленина Карина Анатольевна     https://www.hse.ru/org/persons/202287\n",
       "6      Алова Надежда Владимировна   https://www.hse.ru/org/persons/28123313\n",
       "7     Алферова Татьяна Викторовна  https://www.hse.ru/org/persons/359862051\n",
       "8          Андреева Ольга Юрьевна  https://www.hse.ru/org/persons/152893083\n",
       "9    Андрианов Игорь Владимирович  https://www.hse.ru/org/persons/223725475"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get array of people and links to their profile\n",
    "people=[]\n",
    "for letter in tqdm(letters):\n",
    "    r = requests.get(letter)\n",
    "    page = BeautifulSoup(r.text, 'html.parser')\n",
    "    url = 'https://www.hse.ru'\n",
    "    for link in page.findAll(\"a\", {'class':\"link\"}):\n",
    "        try:\n",
    "            if (link['href'][:13]==\"/org/persons/\" or link['href'][:7]==\"/staff/\") and link['href'][13]!='?' and link.text != 'полный список':\n",
    "                people.append({\n",
    "                    'name': link.text[1:],\n",
    "                    'link': url + link['href']\n",
    "                })\n",
    "        except:\n",
    "            a=1\n",
    "df = pd.DataFrame(people)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "joined-castle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campus = Campus(campus_name=\"Пермь\")\n",
    "campus.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "demonstrated-honolulu",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [13:44<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "#add proffesors to bd\n",
    "vkr = []\n",
    "for person in tqdm(people):\n",
    "    r = requests.get(person['link'])\n",
    "    page = BeautifulSoup(r.text, 'html.parser')\n",
    "    cathedraPerson = page.find(\"ul\", {\"class\":\"g-ul g-list small\"}).findAll(\"a\")[-1].text\n",
    "    positionPerson = page.find(\"span\", {\"class\":\"person-appointment-title\"}).text[:-1]\n",
    "    idCathedra, created = Cathedra.get_or_create(cathedra_name = cathedraPerson)\n",
    "    idPosition, created = Position.get_or_create(position_name = positionPerson)\n",
    "    profName = person['name']\n",
    "    profileLink = person['link']\n",
    "    \n",
    "    prof = Professors(\n",
    "        prof_name = profName,\n",
    "        profile_link = profileLink,\n",
    "        competence = \"\",\n",
    "        embeddings = \"\",\n",
    "        cathedra = idCathedra,\n",
    "        position = idPosition\n",
    "    )\n",
    "    prof.save()\n",
    "    \n",
    "    for link in page.findAll(\"a\", {'class':\"link\"}):\n",
    "            if link['href'][:9]=='/edu/vkr/':\n",
    "                    #print(url+link['href'])\n",
    "                    vkr.append(url+link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "manual-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1790/1790 [1:04:36<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "#get list of vkrs\n",
    "browser = webdriver.Firefox(executable_path = '../.browserDrivers/geckodriver')\n",
    "vkr_data = []\n",
    "for link in tqdm(vkr):\n",
    "    #print(link)\n",
    "    browser.get(link)\n",
    "    browser.implicitly_wait(1)\n",
    "    time.sleep(1)\n",
    "    page = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    vkr_tmp = []\n",
    "    name_vkr=''\n",
    "    name_student=''\n",
    "    name_prof=''\n",
    "    campus=''\n",
    "    program=''\n",
    "    grade=-1\n",
    "    year=-1\n",
    "    link_file=''\n",
    "    for elem in page.findAll(\"h1\"):\n",
    "        #print(elem.text)\n",
    "        name_vkr=elem.text\n",
    "    for elem in page.findAll(\"p\",{'class':'vkr-card__item'}):\n",
    "        #print(elem.text)\n",
    "        elem_name = elem.text.split(':')[0].strip()\n",
    "        elem_text = elem.text.split(':')[1].strip()\n",
    "        if elem_name =='ФИО студента':\n",
    "            name_student = elem_text\n",
    "        elif elem_name =='Руководитель':\n",
    "            name_prof = elem_text\n",
    "        elif elem_name =='Кампус/факультет':\n",
    "            facultElem = elem_text\n",
    "        elif elem_name =='Программа':\n",
    "            programElem = elem_text\n",
    "        elif elem_name =='Год защиты':\n",
    "            year = int(elem_text)\n",
    "        elif elem_name =='Оценка':\n",
    "            grade = int(elem_text)\n",
    "            \n",
    "    facult, created = Faculties.get_or_create(name_faculties = facultElem)\n",
    "    programm, created = EducationalProgram.get_or_create(name_educational_program = programElem)\n",
    "    \n",
    "    for elem in page.findAll(\"a\", {'class':'vkr-icon__text link link_no-underline'}):\n",
    "            link_file=elem['href']\n",
    "    vkrElem = Vkr(\n",
    "        facult = facult,\n",
    "        educational_program = programm,\n",
    "        prof = Professors.get(Professors.prof_name == name_prof),\n",
    "        student = name_student,\n",
    "        vkr_text_link = link_file,\n",
    "        vkr_link = link,\n",
    "        vkr_name = name_vkr,\n",
    "        campus = 1\n",
    "    )    \n",
    "    vkrElem.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alternate-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615/615 [15:48<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "mime = magic.Magic(mime=True)\n",
    "textLinks = Vkr.select().where(Vkr.vkr_text_link != \"\").join(Professors, on=(Vkr.prof == Professors.id))\n",
    "df['fullText'] = \"\"\n",
    "unsuccessful = []\n",
    "successful = []\n",
    "docx = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "pdf = \"application/pdf\"\n",
    "doc = \"application/msword\"\n",
    "for row in tqdm(textLinks):\n",
    "    url = row.vkr_text_link\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('file', 'wb').write(r.content)\n",
    "    fileType = mime.from_file(\"file\")\n",
    "    if fileType == docx:\n",
    "        open('file.docx', 'wb').write(r.content)\n",
    "        f = open('file.docx', 'rb')\n",
    "        document = Document(f)\n",
    "        f.close()\n",
    "        fullText = \"\"\n",
    "        for para in document.paragraphs:\n",
    "            fullText += (para.text)+\" \"\n",
    "        successful.append(url)\n",
    "    elif fileType == doc:\n",
    "        open('file.doc', 'wb').write(r.content)\n",
    "        fullText = textract.process(\"file.doc\").decode(\"utf-8\")\n",
    "        successful.append(url)\n",
    "    elif fileType == pdf:\n",
    "        open('file.pdf', 'wb').write(r.content)\n",
    "        # can't parse latex symbols\n",
    "        try:\n",
    "            fullText = textract.process(\"file.pdf\").decode(\"utf-8\")\n",
    "        except TypeError:\n",
    "            unsuccessful.append(url)\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            unsuccessful.append(url)\n",
    "            continue\n",
    "        successful.append(url)\n",
    "    else:\n",
    "        unsuccessful.append(url)\n",
    "        continue\n",
    "    df.loc[df[\"name\"]==row.prof.prof_name, 'fullText' ] += fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "serious-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"fullText.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "custom_config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "custom_model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased', config=custom_config)\n",
    "custom_model.eval()\n",
    "custom_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "modelSum = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "df['shortText'] = \"\"\n",
    "df['embeddings'] = \"\"\n",
    "for row in tqdm(df.index):\n",
    "    text = df.loc[row, 'fullText'][:1000000]\n",
    "    embeddings = modelSum.run_embeddings(text, num_sentences=10)\n",
    "    result = modelSum(text, num_sentences=10)\n",
    "    summary = \"\".join(result)\n",
    "    df.loc[row, 'shortText'] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distinct-assembly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>shortText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Абашева Александра Сергеевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/485801</td>\n",
       "      <td>Были   поставлены   задачи:   описать   принци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Авраменко Иван Александрович</td>\n",
       "      <td>https://www.hse.ru/org/persons/99247481</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Агаркова Наталия Владиславовна</td>\n",
       "      <td>https://www.hse.ru/org/persons/28123133</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Айхбергер Юрген Томас Германн</td>\n",
       "      <td>https://www.hse.ru/org/persons/359359486</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Алексеева Лариса Николаевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/66770114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                            name  \\\n",
       "0           0             0    Абашева Александра Сергеевна   \n",
       "1           1             1    Авраменко Иван Александрович   \n",
       "2           2             2  Агаркова Наталия Владиславовна   \n",
       "3           3             3   Айхбергер Юрген Томас Германн   \n",
       "4           4             4     Алексеева Лариса Николаевна   \n",
       "\n",
       "                                       link  \\\n",
       "0     https://www.hse.ru/org/persons/485801   \n",
       "1   https://www.hse.ru/org/persons/99247481   \n",
       "2   https://www.hse.ru/org/persons/28123133   \n",
       "3  https://www.hse.ru/org/persons/359359486   \n",
       "4   https://www.hse.ru/org/persons/66770114   \n",
       "\n",
       "                                           shortText  \n",
       "0  Были   поставлены   задачи:   описать   принци...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = pd.read_csv(\"shorttext.csv\")\n",
    "short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "quantitative-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in short.index:\n",
    "    if type(short.loc[row, 'shortText']) == float:\n",
    "        continue\n",
    "    query = Professors.update(competence=short.loc[row, 'shortText']).where(Professors.id == row+1)\n",
    "    query.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "compressed-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'embeddingsPickle'\n",
    "for row in short.index:\n",
    "    if type(short.loc[row, 'shortText']) == float:\n",
    "        continue\n",
    "        \n",
    "    with open(path+'/'+str(row)+\".data\", \"rb\") as f:\n",
    "        arr = pickle.load(f)\n",
    "    strArr = ''.join([str(x) for x in arr])\n",
    "    query = Professors.update(embeddings=strArr).where(Professors.id == row+1)\n",
    "    query.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
