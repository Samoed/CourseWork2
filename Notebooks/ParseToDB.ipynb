{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mysterious-humor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Using cached python-docx-0.8.10.tar.gz (5.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: lxml>=2.3.2 in /home/samoed/.local/lib/python3.8/site-packages (from python-docx) (4.6.3)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.10-py3-none-any.whl size=184489 sha256=926e5e4fd525243a20e0ee67deaffecace72f244ddf6883d5403c45508b7d72b\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/97/4c/2e/68066cbf12b9b2e66403da8982aaf4f656d9f5cb5dc3179e82\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.10\n",
      "Collecting textract\n",
      "  Downloading textract-1.6.3-py3-none-any.whl (21 kB)\n",
      "Collecting xlrd==1.2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting EbookLib==0.17.1\n",
      "  Downloading EbookLib-0.17.1.tar.gz (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting extract-msg==0.23.1\n",
      "  Downloading extract_msg-0.23.1-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 116 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting beautifulsoup4==4.8.0\n",
      "  Downloading beautifulsoup4-4.8.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting python-pptx==0.6.18\n",
      "  Downloading python-pptx-0.6.18.tar.gz (8.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.9 MB 18 kB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet==3.0.4 in /usr/lib/python3/dist-packages (from textract) (3.0.4)\n",
      "Collecting pdfminer.six==20181108\n",
      "  Downloading pdfminer.six-20181108-py2.py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docx2txt==0.8\n",
      "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
      "Collecting argcomplete==1.10.0\n",
      "  Downloading argcomplete-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting SpeechRecognition==3.8.1\n",
      "  Using cached SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: lxml in /home/samoed/.local/lib/python3.8/site-packages (from EbookLib==0.17.1->textract) (4.6.3)\n",
      "Collecting tzlocal==1.5.1\n",
      "  Downloading tzlocal-1.5.1.tar.gz (16 kB)\n",
      "Requirement already satisfied, skipping upgrade: olefile==0.46 in /usr/lib/python3/dist-packages (from extract-msg==0.23.1->textract) (0.46)\n",
      "Collecting imapclient==2.1.0\n",
      "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 611 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: soupsieve>=1.2 in /home/samoed/.local/lib/python3.8/site-packages (from beautifulsoup4==4.8.0->textract) (2.1)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=3.3.2 in /usr/lib/python3/dist-packages (from python-pptx==0.6.18->textract) (7.0.0)\n",
      "Collecting XlsxWriter>=0.5.7\n",
      "  Downloading XlsxWriter-1.4.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 3.8 MB/s eta 0:00:01     |███████▎                        | 440 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /usr/lib/python3/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2019.3)\n",
      "Building wheels for collected packages: EbookLib, python-pptx, docx2txt, tzlocal\n",
      "  Building wheel for EbookLib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for EbookLib: filename=EbookLib-0.17.1-py3-none-any.whl size=38164 sha256=c68b01c39ba4659b9f932285d6807319afbc6956630481371f3d71c929d5f0f7\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/b4/eb/66/00c65b5bbf31ec34329090ec9fe8c8a8d9cb7a3a3d93841386\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.18-py3-none-any.whl size=275703 sha256=ede9047c80e2c122308af2a28d20e8e6e8df5032ef5ed344b1022bf5acaa53f6\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/11/2b/97/d82ca57932fa62d52c723024419c5ec3b7c0f7ecf0a0f06332\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3963 sha256=43dda59c18123894677a420fb8c886cdbc8aee5af82188d1f75f1457168b230b\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/55/f0/2c/81637d42670985178b77df6d41b9b6c6dc18c94818447414b9\n",
      "  Building wheel for tzlocal (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tzlocal: filename=tzlocal-1.5.1-py3-none-any.whl size=17543 sha256=f2aee1cc0f1827d48f56370e35a5277f12acccdefa1240260e337b342756746f\n",
      "  Stored in directory: /home/samoed/.cache/pip/wheels/3a/14/ce/9c504116f6b89e4a05ce0bc0f41983df280d7e00f463481900\n",
      "Successfully built EbookLib python-pptx docx2txt tzlocal\n",
      "Installing collected packages: xlrd, six, EbookLib, tzlocal, imapclient, extract-msg, beautifulsoup4, XlsxWriter, python-pptx, pycryptodome, sortedcontainers, pdfminer.six, docx2txt, argcomplete, SpeechRecognition, textract\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.9.3\n",
      "    Uninstalling beautifulsoup4-4.9.3:\n",
      "      Successfully uninstalled beautifulsoup4-4.9.3\n",
      "Successfully installed EbookLib-0.17.1 SpeechRecognition-3.8.1 XlsxWriter-1.4.0 argcomplete-1.10.0 beautifulsoup4-4.8.0 docx2txt-0.8 extract-msg-0.23.1 imapclient-2.1.0 pdfminer.six-20181108 pycryptodome-3.10.1 python-pptx-0.6.18 six-1.12.0 sortedcontainers-2.3.0 textract-1.6.3 tzlocal-1.5.1 xlrd-1.2.0\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already up-to-date: bert-extractive-summarizer in /home/samoed/.local/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /home/samoed/.local/lib/python3.8/site-packages (from bert-extractive-summarizer) (0.24.1)\n",
      "Requirement already satisfied, skipping upgrade: transformers in /home/samoed/.local/lib/python3.8/site-packages (from bert-extractive-summarizer) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: spacy in /home/samoed/.local/lib/python3.8/site-packages (from bert-extractive-summarizer) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/samoed/.local/lib/python3.8/site-packages (from scikit-learn->bert-extractive-summarizer) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (2021.3.17)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers->bert-extractive-summarizer) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (20.8)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (4.56.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /home/samoed/.local/lib/python3.8/site-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (8.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from spacy->bert-extractive-summarizer) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.1 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.7.3)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /home/samoed/.local/lib/python3.8/site-packages (from spacy->bert-extractive-summarizer) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /home/samoed/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (3.0.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (2021.3.17)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /home/samoed/.local/lib/python3.8/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/samoed/.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/samoed/.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.4.2\n",
      "    Uninstalling transformers-4.4.2:\n",
      "      Successfully uninstalled transformers-4.4.2\n",
      "Successfully installed transformers-4.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U python-docx\n",
    "!pip3 install -U textract\n",
    "!apt-get install antiword\n",
    "!pip3 install -U bert-extractive-summarizer\n",
    "!pip3 install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "artificial-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peewee import *\n",
    "\n",
    "database = SqliteDatabase('bd.db')\n",
    "\n",
    "class BaseModel(Model):\n",
    "    class Meta:\n",
    "        database = database\n",
    "\n",
    "class Campus(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    campus_name = TextField(column_name='Campus_name', null=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Campus'\n",
    "\n",
    "class Cathedra(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    cathedra_name = TextField(column_name='Cathedra_Name', null=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Cathedra'\n",
    "\n",
    "class Faculties(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    name_faculties = TextField(column_name='Name_Faculties', null=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Faculties'\n",
    "\n",
    "class EducationalProgram(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    name_educational_program = TextField(column_name='Name_EducationalProgram', null=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'EducationalProgram'\n",
    "\n",
    "class Position(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    position_name = TextField(column_name='PositionName', null=True)\n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'Position'\n",
    "\n",
    "class Professors(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    prof_name = TextField(column_name='Prof_Name', null=True)\n",
    "    profile_link = TextField(column_name='Profile_Link', null=True)\n",
    "    cathedra = ForeignKeyField(column_name='Cathedra_Id', model=Cathedra, null=True)\n",
    "    position = ForeignKeyField(column_name='Position_Id', model=Position, null=True)\n",
    "    competence = TextField(column_name='Competence', null=True)\n",
    "    embeddings = TextField(column_name='Embeddings', null=True)\n",
    "    \n",
    "    class Meta:\n",
    "        table_name = 'Professors'\n",
    "\n",
    "class Vkr(BaseModel):\n",
    "    id = AutoField(column_name='Id', null=True)\n",
    "    vkr_name = TextField(column_name='VKR_Name', null=True)\n",
    "    vkr_link = TextField(column_name='VKR_Link', null=True)\n",
    "    student = TextField(column_name='Student', null=True)\n",
    "    prof = ForeignKeyField(column_name='Prof_Id', model=Professors, null=True)\n",
    "    campus = ForeignKeyField(column_name='Campus_Id', model=Campus, null=True)\n",
    "    educational_program = ForeignKeyField(column_name='EducationalProgram_Id', model=EducationalProgram, null=True)\n",
    "    vkr_text_link = TextField(column_name='VKRText_Link', null=True)\n",
    "    \n",
    "\n",
    "    class Meta:\n",
    "        table_name = 'VKR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "capable-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateTables():\n",
    "    Campus.create_table()\n",
    "    Cathedra.create_table()\n",
    "    Faculties.create_table()\n",
    "    EducationalProgram.create_table()\n",
    "    Position.create_table()\n",
    "    Professors.create_table()\n",
    "    Vkr.create_table()\n",
    "    \n",
    "CreateTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "compliant-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import time\n",
    "from docx import Document\n",
    "import textract\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "from collections import Counter\n",
    "import string\n",
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secret-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "#get links on letters for people\n",
    "r = requests.get('https://www.hse.ru/org/persons/?udept=135213')\n",
    "page = BeautifulSoup(r.text, 'html.parser')\n",
    "url = 'https://www.hse.ru'\n",
    "letters = []\n",
    "for link in page.findAll(\"div\", {'class': \"abc-filter__letter\"}):\n",
    "    if (str(link.contents)).find('\"') != -1:\n",
    "            s = url + '/org/persons/' + str(link.contents)[10:34]\n",
    "            letters.append(str(s))\n",
    "print(len(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "russian-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:55<00:00,  2.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Абашева Александра Сергеевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/485801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Авраменко Иван Александрович</td>\n",
       "      <td>https://www.hse.ru/org/persons/99247481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Агаркова Наталия Владиславовна</td>\n",
       "      <td>https://www.hse.ru/org/persons/28123133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Айхбергер Юрген Томас Германн</td>\n",
       "      <td>https://www.hse.ru/org/persons/359359486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Алексеева Лариса Николаевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/66770114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Аленина Карина Анатольевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/202287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Алова Надежда Владимировна</td>\n",
       "      <td>https://www.hse.ru/org/persons/28123313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Алферова Татьяна Викторовна</td>\n",
       "      <td>https://www.hse.ru/org/persons/359862051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Андреева Ольга Юрьевна</td>\n",
       "      <td>https://www.hse.ru/org/persons/152893083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Андрианов Игорь Владимирович</td>\n",
       "      <td>https://www.hse.ru/org/persons/223725475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name                                      link\n",
       "0    Абашева Александра Сергеевна     https://www.hse.ru/org/persons/485801\n",
       "1    Авраменко Иван Александрович   https://www.hse.ru/org/persons/99247481\n",
       "2  Агаркова Наталия Владиславовна   https://www.hse.ru/org/persons/28123133\n",
       "3   Айхбергер Юрген Томас Германн  https://www.hse.ru/org/persons/359359486\n",
       "4     Алексеева Лариса Николаевна   https://www.hse.ru/org/persons/66770114\n",
       "5      Аленина Карина Анатольевна     https://www.hse.ru/org/persons/202287\n",
       "6      Алова Надежда Владимировна   https://www.hse.ru/org/persons/28123313\n",
       "7     Алферова Татьяна Викторовна  https://www.hse.ru/org/persons/359862051\n",
       "8          Андреева Ольга Юрьевна  https://www.hse.ru/org/persons/152893083\n",
       "9    Андрианов Игорь Владимирович  https://www.hse.ru/org/persons/223725475"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get array of people and links to their profile\n",
    "people=[]\n",
    "for letter in tqdm(letters):\n",
    "    r = requests.get(letter)\n",
    "    page = BeautifulSoup(r.text, 'html.parser')\n",
    "    url = 'https://www.hse.ru'\n",
    "    for link in page.findAll(\"a\", {'class':\"link\"}):\n",
    "        try:\n",
    "            if (link['href'][:13]==\"/org/persons/\" or link['href'][:7]==\"/staff/\") and link['href'][13]!='?' and link.text != 'полный список':\n",
    "                people.append({\n",
    "                    'name': link.text[1:],\n",
    "                    'link': url + link['href']\n",
    "                })\n",
    "        except:\n",
    "            a=1\n",
    "df = pd.DataFrame(people)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atmospheric-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [11:01<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "#get list of cathedras and positions of people\n",
    "cathedra = set()\n",
    "position = set()\n",
    "for person in tqdm(people):\n",
    "    r = requests.get(person['link'])\n",
    "    page = BeautifulSoup(r.text, 'html.parser')\n",
    "    cathedra.add(page.find(\"ul\", {\"class\":\"g-ul g-list small\"}).findAll(\"a\")[-1].text)\n",
    "    position.add(page.find(\"span\", {\"class\":\"person-appointment-title\"}).text[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "congressional-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "for i in cathedra:\n",
    "    cathedraAdd = Cathedra(cathedra_name=i)\n",
    "    cathedraAdd.save()\n",
    "for i in position:\n",
    "    positionAdd = Position(position_name=i)\n",
    "    positionAdd.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "campus = Campus(campus_name=\"Пермь\")\n",
    "campus.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demonstrated-honolulu",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [09:07<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "#add proffesors to bd\n",
    "for person in tqdm(people):\n",
    "    r = requests.get(person['link'])\n",
    "    page = BeautifulSoup(r.text, 'html.parser')\n",
    "    cathedraPerson = page.find(\"ul\", {\"class\":\"g-ul g-list small\"}).findAll(\"a\")[-1].text\n",
    "    positionPerson = page.find(\"span\", {\"class\":\"person-appointment-title\"}).text[:-1]\n",
    "    idCathedra = Cathedra.get(Cathedra.cathedra_name == cathedraPerson)\n",
    "    idPosition = Position.get(Position.position_name == positionPerson)\n",
    "    profName = person['name']\n",
    "    profileLink = person['link']\n",
    "    \n",
    "    prof = Professors(\n",
    "        cathedra = idCathedra,\n",
    "        competence = \"\",\n",
    "        embeddings = \"\",\n",
    "        position = idPosition,\n",
    "        prof_name = profName,\n",
    "        profile_link = profileLink\n",
    "    )\n",
    "    prof.save()\n",
    "    \n",
    "    for link in page.findAll(\"a\", {'class':\"link\"}):\n",
    "            if link['href'][:9]=='/edu/vkr/':\n",
    "                    #print(url+link['href'])\n",
    "                    vkr.append(url+link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cooked-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1790/1790 [51:13<00:00,  1.72s/it] \n"
     ]
    }
   ],
   "source": [
    "#get list of OP and facultets\n",
    "browser = webdriver.Firefox(executable_path = '../.browserDrivers/geckodriver')\n",
    "OP = set()\n",
    "facult = set()\n",
    "for link in tqdm(vkr):\n",
    "    browser.get(link)\n",
    "    browser.implicitly_wait(1)\n",
    "    time.sleep(1)\n",
    "    page = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    for elem in page.findAll(\"p\",{'class':'vkr-card__item'}):\n",
    "        elem_text = elem.text.split(':')[1].strip()\n",
    "        elem_name = elem.text.split(':')[0].strip()\n",
    "        #print(elem_name)\n",
    "        #print(elem_text)\n",
    "        if elem_name =='Кампус/факультет':\n",
    "            #print(elem_name)\n",
    "            facult.add(elem_text)\n",
    "        elif elem_name =='Программа':\n",
    "            #print(elem_name)\n",
    "            OP.add(elem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "professional-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "for i in facult:\n",
    "    facultAdd = Faculties(name_faculties=i)\n",
    "    facultAdd.save()\n",
    "    \n",
    "for i in OP:\n",
    "    facultAdd = EducationalProgram(name_educational_program=i)\n",
    "    facultAdd.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "manual-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 27/1790 [00:57<1:02:41,  2.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-925e10b967ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvkr_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#get list of vkrs\n",
    "browser = webdriver.Firefox(executable_path = '../.browserDrivers/geckodriver')\n",
    "vkr_data = []\n",
    "for link in tqdm(vkr):\n",
    "    #print(link)\n",
    "    browser.get(link)\n",
    "    browser.implicitly_wait(1)\n",
    "    time.sleep(1)\n",
    "    page = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    vkr_tmp = []\n",
    "    name_vkr=''\n",
    "    name_student=''\n",
    "    name_prof=''\n",
    "    campus=''\n",
    "    program=''\n",
    "    grade=-1\n",
    "    year=-1\n",
    "    link_file=''\n",
    "    for elem in page.findAll(\"h1\"):\n",
    "        #print(elem.text)\n",
    "        name_vkr=elem.text\n",
    "    for elem in page.findAll(\"p\",{'class':'vkr-card__item'}):\n",
    "        #print(elem.text)\n",
    "        elem_name = elem.text.split(':')[0].strip()\n",
    "        elem_text = elem.text.split(':')[1].strip()\n",
    "        if elem_name =='ФИО студента':\n",
    "            name_student = elem_text\n",
    "        elif elem_name =='Руководитель':\n",
    "            name_prof = elem_text\n",
    "        elif elem_name =='Кампус/факультет':\n",
    "            facultElem = elem_text\n",
    "        elif elem_name =='Программа':\n",
    "            program = elem_text\n",
    "        elif elem_name =='Год защиты':\n",
    "            year = int(elem_text)\n",
    "        elif elem_name =='Оценка':\n",
    "            grade = int(elem_text)\n",
    "            \n",
    "    for elem in page.findAll(\"a\", {'class':'vkr-icon__text link link_no-underline'}):\n",
    "            link_file=elem['href']\n",
    "    vkrElem = Vkr(\n",
    "        facult = Faculties.get(Faculties.name_faculties == facultElem),\n",
    "        educational_program = EducationalProgram.get(EducationalProgram.name_educational_program == program),\n",
    "        prof = Professors.get(Professors.prof_name == name_prof),\n",
    "        student = name_student,\n",
    "        vkr_text_link = link_file,\n",
    "        vkr_link = link,\n",
    "        vkr_name = name_vkr,\n",
    "        campus = 1\n",
    "    )    \n",
    "    vkrElem.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "harmful-animation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lms.hse.ru/ap_service.php?getwork=1&guid=7FBCBD17-2BD2-4324-9320-DA9AB18846B7\n",
      "https://www.hse.ru/data/2014/05/19/1322118389/Диссертация_19Мая_отправка.docx\n",
      "https://www.hse.ru/data/2014/05/21/1321758023/Пугачевский А.В. Разработка коммерческог.. Кондитерская фабрика Пермская.docx\n",
      "https://www.hse.ru/data/2013/08/21/1289969400/Тужанская Елена.doc\n",
      "https://lms.hse.ru/ap_service.php?getwork=1&guid=7F05C602-D501-405F-ABA5-CD3D3DFE300D\n",
      "https://lms.hse.ru/ap_service.php?getwork=1&guid=35592BC0-9CE0-4F71-B877-D31B060DE9F5\n",
      "https://lms.hse.ru/ap_service.php?getwork=1&guid=7F05C602-D501-405F-ABA5-CD3D3DFE300D\n"
     ]
    }
   ],
   "source": [
    "textLinks = Vkr.select().where(Vkr.vkr_text_link != \"\").join(Professors, on=(Vkr.prof == Professors.id))\n",
    "for i in textLinks:\n",
    "    print(i.vkr_text_link)\n",
    "len(textLinks)\n",
    "print(textLinks[4].vkr_text_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "applied-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "339    False\n",
       "340    False\n",
       "341    False\n",
       "342    False\n",
       "343    False\n",
       "Name: name, Length: 344, dtype: bool"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"name\"]==textLinks[4].prof.prof_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "alternate-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "textLinks = Vkr.select().where(Vkr.vkr_text_link != \"\").join(Professors, on=(Vkr.prof == Professors.id))\n",
    "df['fullText'] = \"\"\n",
    "unsuccessful = []\n",
    "successful = []\n",
    "for row in tqdm(textLinks):\n",
    "    url = row.vkr_text_link\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    try:\n",
    "        open('test.docx', 'wb').write(r.content)\n",
    "        f = open('test.docx', 'rb')\n",
    "        document = Document(f)\n",
    "        f.close()\n",
    "        successful.append(url)\n",
    "    except ValueError:\n",
    "        unsuccessful.append(url)\n",
    "        continue\n",
    "        \n",
    "        fullText = \"\"\n",
    "        for para in document.paragraphs:\n",
    "            fullText+=(para.text)+\" \"\n",
    "        df.loc[df[\"name\"]==row.prof.prof_name, 'fullText' ] = fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "medical-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "unsuccessful = []\n",
    "for row in tqdm(textLinks):\n",
    "    url = row.vkr_text_link\n",
    "    if url in successful:\n",
    "        continue\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    try:\n",
    "        open('test.doc', 'wb').write(r.content)\n",
    "        df.loc[df[\"name\"]==row.prof.prof_name, 'fullText' ] = textract.process(\"test.doc\").decode(\"utf-8\")\n",
    "        successful.append(url)\n",
    "    except ValueError:\n",
    "        unsuccessful.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "technical-grave",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ShellError",
     "evalue": "The command `pdftotext test.pdf -` failed with exit code 1\n------------- stdout -------------\nb''------------- stderr -------------\nb\"Syntax Warning: May not be a PDF file (continuing anyway)\\nSyntax Error (26): Illegal character '>'\\nSyntax Error: Couldn't find trailer dictionary\\nSyntax Error: Couldn't find trailer dictionary\\nSyntax Error: Couldn't read xref table\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-bdb919be1287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprof_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fullText'\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0munsuccessful\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/textract/parsers/__init__.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(filename, encoding, extension, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiletype_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/textract/parsers/utils.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, filename, encoding, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# output encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mbyte_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0municode_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/textract/parsers/pdf_parser.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_pdfminer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pdfminer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/textract/parsers/pdf_parser.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pdftotext'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_pdftotext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mShellError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# If pdftotext isn't installed and the pdftotext method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/textract/parsers/pdf_parser.py\u001b[0m in \u001b[0;36mextract_pdftotext\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'pdftotext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/textract/parsers/utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# if pipe is busted, raise an error (unlike Fabric)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             raise exceptions.ShellError(\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n",
      "\u001b[0;31mShellError\u001b[0m: The command `pdftotext test.pdf -` failed with exit code 1\n------------- stdout -------------\nb''------------- stderr -------------\nb\"Syntax Warning: May not be a PDF file (continuing anyway)\\nSyntax Error (26): Illegal character '>'\\nSyntax Error: Couldn't find trailer dictionary\\nSyntax Error: Couldn't find trailer dictionary\\nSyntax Error: Couldn't read xref table\\n\""
     ]
    }
   ],
   "source": [
    "unsuccessful = []\n",
    "for row in tqdm(textLinks):\n",
    "    url = row.vkr_text_link\n",
    "    if url in successful:\n",
    "        continue\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('test.pdf', 'wb').write(r.content)\n",
    "    try:\n",
    "        df.loc[df[\"name\"]==row.prof.prof_name, 'fullText' ] = textract.process(\"test.pdf\").decode(\"utf-8\")\n",
    "    except ValueError:\n",
    "        unsuccessful.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "custom_config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "custom_model = BertModel.from_pretrained('DeepPavlov/rubert-base-cased', config=custom_config)\n",
    "custom_model.eval()\n",
    "custom_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "modelSum = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "df['shortText'] = \"\"\n",
    "df['embeddings'] = \"\"\n",
    "for row in tqdm(df.index):\n",
    "    text = df.loc[row, 'fullText'][:1000000]\n",
    "    result = modelSum(text, num_sentences=10)\n",
    "    embeddings = modelSum.run_embeddings(text, num_sentences=10)\n",
    "    summary = \"\".join(result)\n",
    "    df.loc[row, 'shortText'] = summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
